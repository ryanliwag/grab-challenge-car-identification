{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import utils\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Load_Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = \"data/cars_train/\"\n",
    "car_annotations_path = \"data/devkit/cars_train_annos.mat\"\n",
    "car_metadata_path = \"data/devkit/cars_meta.mat\"\n",
    "\n",
    "#Load Meta Data\n",
    "meta_data = loadmat(car_metadata_path)\n",
    "meta_data = np.concatenate(meta_data[\"class_names\"][0])\n",
    "\n",
    "nb_classes = len(meta_data)\n",
    "\n",
    "dataset = Load_Images(root_dir = root_dir, annotations_path=car_annotations_path, seed=seed, train_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class car_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, files, root_dir, meta_data, image_transform=None):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "        #image file names\n",
    "        self.image_files = [file[-1][0] for file in files]\n",
    "        \n",
    "        #Class ID\n",
    "        #id needs to be adjusted by 1, for pytorch NLLosss \n",
    "        self.id = [file[-2][0] - 1 for file in files]\n",
    "        \n",
    "        #Class Name\n",
    "        self.class_name = [meta_data[file[-2][0] - 1][0] for file in files]\n",
    "        \n",
    "        #Get Car Year\n",
    "        self.carYear, self.carYear_ID = utils.get_Year(self.class_name)\n",
    "        \n",
    "        #Get Car Maker\n",
    "        self.carMaker, self.carMaker_ID = utils.get_Maker(self.class_name)\n",
    "        \n",
    "        #Get Car Type\n",
    "        self.carType, self.carType_ID = utils.get_Type(self.class_name)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        if self.image_transform:\n",
    "            img = self.image_transform(img)\n",
    "        \n",
    "        target = torch.from_numpy(np.array(self.id[idx]))[0]\n",
    "\n",
    "        sample = {'Image':img, 'class_ID':target, \"class_name\":self.class_name[idx],\n",
    "                 'year_ID':self.carYear_ID[idx], 'maker_ID':self.carMaker_ID[idx],\n",
    "                 'type_ID':self.carType_ID[idx]}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, class_type):\n",
    "                      \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        \n",
    "        self.num_samples = len(self.indices) \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx, class_type)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx, class_type)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx, class_type):\n",
    "        return dataset[idx][class_type].item()\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "                self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Training Weighted Random Sampler\n",
    "targets = [i[-2][0][0] for i in dataset[\"training\"]]\n",
    "class_sample_counts=[len(np.where(targets == t)[0]) for t in np.unique(targets)]\n",
    "weight = 1. / np.array(class_sample_counts)\n",
    "samples_weight = np.array([weight[t-1] for t in targets])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weight = samples_weight.double()\n",
    "training_sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=False)\n",
    "\n",
    "#batch size\n",
    "bs = 15\n",
    "\n",
    "image_transformers = {'train': transforms.Compose([transforms.Resize((244,244)),\n",
    "                                                   transforms.RandomRotation(degrees=50),\n",
    "                                                   transforms.RandomHorizontalFlip(0.8),\n",
    "                                                   transforms.RandomPerspective(),\n",
    "                                                   transforms.RandomResizedCrop(size=((244,244)), scale = (0.8, 1.0)),\n",
    "                                                   transforms.ColorJitter(brightness=0.8, contrast=0.8),\n",
    "                                                   transforms.ToTensor()]),\n",
    "                      'validation': transforms.Compose([transforms.Resize((244,244)),\n",
    "                                                       transforms.ToTensor()\n",
    "                                                       ])\n",
    "                     }\n",
    "\n",
    "#\n",
    "training_data = car_dataset(dataset[\"training\"],\n",
    "                            root_dir = root_dir,\n",
    "                            meta_data = meta_data,\n",
    "                            image_transform = image_transformers[\"train\"]\n",
    "                           )\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=bs, \n",
    "                                           sampler=ImbalancedDatasetSampler(training_data, \"year_ID\"))\n",
    "\n",
    "validation_data = car_dataset(dataset[\"validation\"], \n",
    "                             root_dir = root_dir,\n",
    "                             meta_data = meta_data,\n",
    "                             image_transform  = image_transformers[\"validation\"])\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=bs,\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ids = [i[\"year_ID\"] for i in training_data]\n",
    "#validation_ids = [i[\"class_name\"] for i in validation_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(pd.Series(training_ids).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: GeForce GTX 960\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=196, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Load the model based on VGG19\n",
    "vgg_based = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "'''\n",
    "for param in vgg_based.parameters():\n",
    "    param.requires_grad = False\n",
    "'''\n",
    "\n",
    "\n",
    "for idx,param in enumerate(vgg_based.parameters()):\n",
    "    if idx <= 35:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "# Modify the last layer\n",
    "number_features = vgg_based.classifier[6].in_features\n",
    "features = list(vgg_based.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([torch.nn.Linear(number_features, nb_classes)])\n",
    "vgg_based.classifier = torch.nn.Sequential(*features)\n",
    "\n",
    "vgg_based = vgg_based.to(device)\n",
    "\n",
    "print(vgg_based)\n",
    "#torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0)\n",
    "#\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = torch.optim.Adam(vgg_based.parameters(), lr= 0.0005, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_ft = torch.optim.Adam(vgg_based.parameters(), lr= 0.001, weight_decay=1e-5)\n",
    "#torch.optim.Adam(vgg_based.parameters(), lr= 0.0001, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=1):\n",
    "    since = time.time()\n",
    "    history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 30)\n",
    "\n",
    "        training_loss = 0\n",
    "        validation_loss = 0\n",
    "        \n",
    "        training_accuracy = 0\n",
    "        validation_accuracy = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            inputs = data[\"Image\"]\n",
    "            labels = data[\"year_ID\"]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs  = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            training_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            #train accuracy\n",
    "            (max_vals, arg_maxs) = torch.max(outputs, dim=1) \n",
    "            correct_counts = arg_maxs.eq(labels.data.view_as(arg_maxs))\n",
    "\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            training_accuracy += acc.item() * inputs.size(0)\n",
    "\n",
    "            \n",
    "        #get accuracy\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for batch_idx, data in enumerate(validation_loader):\n",
    "                inputs = data[\"Image\"]\n",
    "                labels = data[\"year_ID\"]\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.long()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                validation_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                #train accuracy\n",
    "                (max_vals, arg_maxs) = torch.max(outputs, dim=1) \n",
    "                correct_counts = arg_maxs.eq(labels.data.view_as(arg_maxs))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                validation_accuracy += acc.item() * inputs.size(0)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = training_loss/len(training_data)\n",
    "        avg_train_acc = training_accuracy/float(len(training_data))\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = validation_loss/len(validation_data)\n",
    "        avg_valid_acc = validation_accuracy/float(len(validation_data))\n",
    "        history.append([avg_train_loss, avg_train_acc, avg_valid_loss, avg_valid_acc])\n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%\".format(epoch + 1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "------------------------------\n",
      "Epoch : 001, Training: Loss: 2.1971, Accuracy: 32.2308%, \n",
      "\t\tValidation : Loss : 2.2723, Accuracy: 19.6187%\n",
      "Epoch 2/50\n",
      "------------------------------\n",
      "Epoch : 002, Training: Loss: 1.7278, Accuracy: 45.2615%, \n",
      "\t\tValidation : Loss : 2.4672, Accuracy: 14.1451%\n",
      "Epoch 3/50\n",
      "------------------------------\n",
      "Epoch : 003, Training: Loss: 1.5976, Accuracy: 49.2923%, \n",
      "\t\tValidation : Loss : 2.1575, Accuracy: 25.3383%\n",
      "Epoch 4/50\n",
      "------------------------------\n",
      "Epoch : 004, Training: Loss: 1.5416, Accuracy: 50.3538%, \n",
      "\t\tValidation : Loss : 2.1376, Accuracy: 20.9102%\n",
      "Epoch 5/50\n",
      "------------------------------\n",
      "Epoch : 005, Training: Loss: 1.4554, Accuracy: 53.6154%, \n",
      "\t\tValidation : Loss : 2.2610, Accuracy: 17.8352%\n",
      "Epoch 6/50\n",
      "------------------------------\n",
      "Epoch : 006, Training: Loss: 1.3816, Accuracy: 56.1231%, \n",
      "\t\tValidation : Loss : 2.1858, Accuracy: 20.6027%\n",
      "Epoch 7/50\n",
      "------------------------------\n",
      "Epoch : 007, Training: Loss: 1.3351, Accuracy: 57.0923%, \n",
      "\t\tValidation : Loss : 2.1390, Accuracy: 22.7552%\n",
      "Epoch 8/50\n",
      "------------------------------\n",
      "Epoch : 008, Training: Loss: 1.2640, Accuracy: 59.5231%, \n",
      "\t\tValidation : Loss : 2.1038, Accuracy: 23.2472%\n",
      "Epoch 9/50\n",
      "------------------------------\n",
      "Epoch : 009, Training: Loss: 1.2876, Accuracy: 58.7846%, \n",
      "\t\tValidation : Loss : 1.8536, Accuracy: 29.9508%\n",
      "Epoch 10/50\n",
      "------------------------------\n",
      "Epoch : 010, Training: Loss: 1.2420, Accuracy: 60.0308%, \n",
      "\t\tValidation : Loss : 1.8635, Accuracy: 35.0554%\n",
      "Epoch 11/50\n",
      "------------------------------\n",
      "Epoch : 011, Training: Loss: 1.2276, Accuracy: 59.9846%, \n",
      "\t\tValidation : Loss : 1.9046, Accuracy: 31.9188%\n",
      "Epoch 12/50\n",
      "------------------------------\n",
      "Epoch : 012, Training: Loss: 1.1763, Accuracy: 62.2462%, \n",
      "\t\tValidation : Loss : 1.8672, Accuracy: 34.3788%\n",
      "Epoch 13/50\n",
      "------------------------------\n",
      "Epoch : 013, Training: Loss: 1.1749, Accuracy: 61.1231%, \n",
      "\t\tValidation : Loss : 1.8010, Accuracy: 35.2399%\n",
      "Epoch 14/50\n",
      "------------------------------\n",
      "Epoch : 014, Training: Loss: 1.1759, Accuracy: 62.1231%, \n",
      "\t\tValidation : Loss : 2.0623, Accuracy: 27.2448%\n",
      "Epoch 15/50\n",
      "------------------------------\n",
      "Epoch : 015, Training: Loss: 1.1372, Accuracy: 63.5846%, \n",
      "\t\tValidation : Loss : 1.9448, Accuracy: 28.8438%\n",
      "Epoch 16/50\n",
      "------------------------------\n",
      "Epoch : 016, Training: Loss: 1.1223, Accuracy: 63.0308%, \n",
      "\t\tValidation : Loss : 1.8700, Accuracy: 31.8573%\n",
      "Epoch 17/50\n",
      "------------------------------\n",
      "Epoch : 017, Training: Loss: 1.1010, Accuracy: 64.1846%, \n",
      "\t\tValidation : Loss : 1.9612, Accuracy: 30.7503%\n",
      "Epoch 18/50\n",
      "------------------------------\n",
      "Epoch : 018, Training: Loss: 1.1053, Accuracy: 64.3692%, \n",
      "\t\tValidation : Loss : 1.6738, Accuracy: 40.4674%\n",
      "Epoch 19/50\n",
      "------------------------------\n",
      "Epoch : 019, Training: Loss: 1.0700, Accuracy: 65.5692%, \n",
      "\t\tValidation : Loss : 1.6843, Accuracy: 40.2214%\n",
      "Epoch 20/50\n",
      "------------------------------\n",
      "Epoch : 020, Training: Loss: 1.0394, Accuracy: 66.9538%, \n",
      "\t\tValidation : Loss : 1.8826, Accuracy: 32.4723%\n",
      "Epoch 21/50\n",
      "------------------------------\n",
      "Epoch : 021, Training: Loss: 1.0298, Accuracy: 66.8000%, \n",
      "\t\tValidation : Loss : 1.4449, Accuracy: 50.5535%\n",
      "Epoch 22/50\n",
      "------------------------------\n",
      "Epoch : 022, Training: Loss: 1.0437, Accuracy: 66.6308%, \n",
      "\t\tValidation : Loss : 2.1108, Accuracy: 25.0308%\n",
      "Epoch 23/50\n",
      "------------------------------\n",
      "Epoch : 023, Training: Loss: 1.0268, Accuracy: 66.6615%, \n",
      "\t\tValidation : Loss : 1.7197, Accuracy: 38.3764%\n",
      "Epoch 24/50\n",
      "------------------------------\n",
      "Epoch : 024, Training: Loss: 1.0152, Accuracy: 66.4462%, \n",
      "\t\tValidation : Loss : 1.8423, Accuracy: 34.8093%\n",
      "Epoch 25/50\n",
      "------------------------------\n",
      "Epoch : 025, Training: Loss: 1.0328, Accuracy: 66.0308%, \n",
      "\t\tValidation : Loss : 1.9662, Accuracy: 30.7503%\n",
      "Epoch 26/50\n",
      "------------------------------\n",
      "Epoch : 026, Training: Loss: 1.0272, Accuracy: 66.4154%, \n",
      "\t\tValidation : Loss : 2.1180, Accuracy: 28.2288%\n",
      "Epoch 27/50\n",
      "------------------------------\n",
      "Epoch : 027, Training: Loss: 1.0171, Accuracy: 66.6462%, \n",
      "\t\tValidation : Loss : 1.9653, Accuracy: 31.2423%\n",
      "Epoch 28/50\n",
      "------------------------------\n",
      "Epoch : 028, Training: Loss: 0.9874, Accuracy: 67.1846%, \n",
      "\t\tValidation : Loss : 2.1548, Accuracy: 26.5068%\n",
      "Epoch 29/50\n",
      "------------------------------\n",
      "Epoch : 029, Training: Loss: 1.0018, Accuracy: 67.4000%, \n",
      "\t\tValidation : Loss : 1.9971, Accuracy: 29.9508%\n",
      "Epoch 30/50\n",
      "------------------------------\n",
      "Epoch : 030, Training: Loss: 0.9817, Accuracy: 68.0308%, \n",
      "\t\tValidation : Loss : 2.0132, Accuracy: 29.5818%\n",
      "Epoch 31/50\n",
      "------------------------------\n",
      "Epoch : 031, Training: Loss: 0.9743, Accuracy: 68.4462%, \n",
      "\t\tValidation : Loss : 1.9663, Accuracy: 30.9963%\n",
      "Epoch 32/50\n",
      "------------------------------\n",
      "Epoch : 032, Training: Loss: 0.9584, Accuracy: 68.7846%, \n",
      "\t\tValidation : Loss : 2.0066, Accuracy: 30.0123%\n",
      "Epoch 33/50\n",
      "------------------------------\n",
      "Epoch : 033, Training: Loss: 0.9495, Accuracy: 68.4308%, \n",
      "\t\tValidation : Loss : 2.0058, Accuracy: 30.6888%\n",
      "Epoch 34/50\n",
      "------------------------------\n",
      "Epoch : 034, Training: Loss: 0.9526, Accuracy: 69.0769%, \n",
      "\t\tValidation : Loss : 1.6624, Accuracy: 40.5289%\n",
      "Epoch 35/50\n",
      "------------------------------\n",
      "Epoch : 035, Training: Loss: 0.9454, Accuracy: 69.0769%, \n",
      "\t\tValidation : Loss : 1.9246, Accuracy: 32.4723%\n",
      "Epoch 36/50\n",
      "------------------------------\n",
      "Epoch : 036, Training: Loss: 0.9448, Accuracy: 68.8462%, \n",
      "\t\tValidation : Loss : 1.6302, Accuracy: 42.0049%\n",
      "Epoch 37/50\n",
      "------------------------------\n",
      "Epoch : 037, Training: Loss: 0.9230, Accuracy: 69.8308%, \n",
      "\t\tValidation : Loss : 1.7099, Accuracy: 38.1304%\n",
      "Epoch 38/50\n",
      "------------------------------\n",
      "Epoch : 038, Training: Loss: 0.9452, Accuracy: 68.4462%, \n",
      "\t\tValidation : Loss : 1.8661, Accuracy: 33.7023%\n",
      "Epoch 39/50\n",
      "------------------------------\n",
      "Epoch : 039, Training: Loss: 0.9514, Accuracy: 69.2154%, \n",
      "\t\tValidation : Loss : 2.0577, Accuracy: 28.1058%\n",
      "Epoch 40/50\n",
      "------------------------------\n",
      "Epoch : 040, Training: Loss: 0.9511, Accuracy: 69.0154%, \n",
      "\t\tValidation : Loss : 2.1229, Accuracy: 26.9988%\n",
      "Epoch 41/50\n",
      "------------------------------\n",
      "Epoch : 041, Training: Loss: 0.9261, Accuracy: 69.3692%, \n",
      "\t\tValidation : Loss : 1.8049, Accuracy: 35.4859%\n",
      "Epoch 42/50\n",
      "------------------------------\n",
      "Epoch : 042, Training: Loss: 0.9069, Accuracy: 70.1846%, \n",
      "\t\tValidation : Loss : 1.6012, Accuracy: 40.8979%\n",
      "Epoch 43/50\n",
      "------------------------------\n",
      "Epoch : 043, Training: Loss: 0.8976, Accuracy: 70.5538%, \n",
      "\t\tValidation : Loss : 1.9317, Accuracy: 32.9643%\n",
      "Epoch 44/50\n",
      "------------------------------\n",
      "Epoch : 044, Training: Loss: 0.9602, Accuracy: 68.4000%, \n",
      "\t\tValidation : Loss : 1.8129, Accuracy: 36.7774%\n",
      "Epoch 45/50\n",
      "------------------------------\n",
      "Epoch : 045, Training: Loss: 0.9109, Accuracy: 71.0615%, \n",
      "\t\tValidation : Loss : 1.7894, Accuracy: 35.9779%\n",
      "Epoch 46/50\n",
      "------------------------------\n",
      "Epoch : 046, Training: Loss: 0.9126, Accuracy: 69.2308%, \n",
      "\t\tValidation : Loss : 1.9985, Accuracy: 30.0123%\n",
      "Epoch 47/50\n",
      "------------------------------\n",
      "Epoch : 047, Training: Loss: 0.9185, Accuracy: 69.0923%, \n",
      "\t\tValidation : Loss : 1.6412, Accuracy: 43.2349%\n",
      "Epoch 48/50\n",
      "------------------------------\n",
      "Epoch : 048, Training: Loss: 0.9029, Accuracy: 70.8154%, \n",
      "\t\tValidation : Loss : 1.7519, Accuracy: 37.5769%\n",
      "Epoch 49/50\n",
      "------------------------------\n",
      "Epoch : 049, Training: Loss: 0.9216, Accuracy: 69.7385%, \n",
      "\t\tValidation : Loss : 1.7039, Accuracy: 40.7749%\n",
      "Epoch 50/50\n",
      "------------------------------\n",
      "Epoch : 050, Training: Loss: 0.8831, Accuracy: 70.7231%, \n",
      "\t\tValidation : Loss : 1.8178, Accuracy: 33.5793%\n",
      "Training complete in 258m 9s\n"
     ]
    }
   ],
   "source": [
    "mods, history = train_model(vgg_based, criterion, optimizer_ft, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    yhats = []  \n",
    "    ys = []\n",
    "    mods.eval()\n",
    "\n",
    "    for batch_idx, data in enumerate(validation_loader):\n",
    "        inputs = data[\"Image\"]\n",
    "        labels = data[\"class_ID\"]\n",
    "        ys.append(labels)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.long()\n",
    "        yhats.append(mods(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in yhats:\n",
    "    for g in i:\n",
    "        _, ind = torch.max(g, 0)\n",
    "        preds.append(ind.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values, indices = torch.max(yhats[0], 0)\n",
    "print(values, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for i in ys:\n",
    "    for g in i.numpy():\n",
    "        targets.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26504132e80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history, columns=[\"avg_train_loss\", \"avg_train_acc\", \"avg_valid_loss\", \"avg_valid_acc\"])[[\"avg_valid_acc\", \"avg_train_acc\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_image_name):\n",
    "     \n",
    "    transform = image_transforms['test']\n",
    " \n",
    "    test_image = Image.open(test_image_name)\n",
    "    plt.imshow(test_image)\n",
    "     \n",
    "    test_image_tensor = transform(test_image)\n",
    " \n",
    "    if torch.cuda.is_available():\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
    "    else:\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_image_tensor)\n",
    "        ps = torch.exp(out)\n",
    "        topk, topclass = ps.topk(1, dim=1)\n",
    "        print(\"Output class :  \", idx_to_class[topclass.cpu().numpy()[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class CustomDatasetFromCSV(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.labels = pd.get_dummies(self.data['emotion']).as_matrix()\n",
    "        self.height = 48\n",
    "        self.width = 48\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # This method should return only 1 sample and label \n",
    "        # (according to \"index\"), not the whole dataset\n",
    "        # So probably something like this for you:\n",
    "        pixel_sequence = self.data['pixels'][index]\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        face = np.asarray(face).reshape(self.width, self.height)\n",
    "        face = cv2.resize(face.astype('uint8'), (self.width, self.height))\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return face, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "dataset = CustomDatasetFromCSV(my_path)\n",
    "batch_size = 16\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "# Usage Example:\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train:   \n",
    "    for batch_index, (faces, labels) in enumerate(train_loader):\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "            \n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        dataset_type = type(dataset)\n",
    "        if dataset_type is torchvision.datasets.MNIST:\n",
    "            return dataset.train_labels[idx].item()\n",
    "        elif dataset_type is torchvision.datasets.ImageFolder:\n",
    "            return dataset.imgs[idx][1]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_dir = \"alien_pred\"\n",
    "input_shape = 224\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "#data transformation\n",
    "data_transforms = {\n",
    "   'train': transforms.Compose([\n",
    "       transforms.CenterCrop(input_shape),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean, std)\n",
    "   ]),\n",
    "   'validation': transforms.Compose([\n",
    "       transforms.CenterCrop(input_shape),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean, std)\n",
    "   ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "   x: datasets.ImageFolder(\n",
    "       os.path.join(data_dir, x),\n",
    "       transform=data_transforms[x]\n",
    "   )\n",
    "   for x in ['train', 'validation']\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "   x: torch.utils.data.DataLoader(\n",
    "       image_datasets[x], batch_size=32,\n",
    "       shuffle=True, num_workers=4\n",
    "   )\n",
    "   for x in ['train', 'validation']\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation']}\n",
    "\n",
    "print(dataset_sizes)\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(vgg_based.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "   since = time.time()\n",
    "\n",
    "   for epoch in range(num_epochs):\n",
    "       print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "       print('-' * 10)\n",
    "\n",
    "       #set model to trainable\n",
    "       # model.train()\n",
    "\n",
    "       train_loss = 0\n",
    "\n",
    "       # Iterate over data.\n",
    "       for i, data in enumerate(dataloaders['train']):\n",
    "           inputs , labels = data\n",
    "           inputs = inputs.to(device)\n",
    "           labels = labels.to(device)\n",
    "\n",
    "           optimizer.zero_grad()\n",
    "          \n",
    "           with torch.set_grad_enabled(True):\n",
    "               outputs  = model(inputs)\n",
    "               loss = criterion(outputs, labels)\n",
    "\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "\n",
    "           train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "           print('{} Loss: {:.4f}'.format(\n",
    "               'train', train_loss / dataset_sizes['train']))\n",
    "          \n",
    "   time_elapsed = time.time() - since\n",
    "   print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "       time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "   return model\n",
    "\n",
    "def visualize_model(model, num_images=6):\n",
    "   was_training = model.training\n",
    "   model.eval()\n",
    "   images_so_far = 0\n",
    "   fig = plt.figure()\n",
    "\n",
    "   with torch.no_grad():\n",
    "       for i, (inputs, labels) in enumerate(dataloaders['validation']):\n",
    "           inputs = inputs.to(device)\n",
    "           labels = labels.to(device)\n",
    "\n",
    "           outputs = model(inputs)\n",
    "           _, preds = torch.max(outputs, 1)\n",
    "\n",
    "           for j in range(inputs.size()[0]):\n",
    "               images_so_far += 1\n",
    "               ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "               ax.axis('off')\n",
    "               ax.set_title('predicted: {} truth: {}'.format(class_names[preds[j]], class_names[labels[j]]))\n",
    "               img = inputs.cpu().data[j].numpy().transpose((1, 2, 0))\n",
    "               img = std * img + mean\n",
    "               ax.imshow(img)\n",
    "\n",
    "               if images_so_far == num_images:\n",
    "                   model.train(mode=was_training)\n",
    "                   return\n",
    "       model.train(mode=was_training)\n",
    "    \n",
    "vgg_based = train_model(vgg_based, criterion, optimizer_ft, num_epochs=25)\n",
    "\n",
    "visualize_model(vgg_based)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
