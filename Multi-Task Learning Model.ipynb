{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import utils\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#using GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "class car_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Pytorch Dataset class for reading Car Dataset meta files and images.\n",
    "    Arguments:\n",
    "        files (list, required): \n",
    "        root_dir (str, required): Root directory path\n",
    "        meta_data (list, required): exctracted meta data\n",
    "    Returns a Dictionary\n",
    "    \"\"\"\n",
    "    def __init__(self, files, root_dir, meta_data, image_transform=None):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "        #image file names\n",
    "        self.image_files = [file[-1][0] for file in files]\n",
    "        \n",
    "        #Class ID\n",
    "        self.id = [file[-2][0] - 1 for file in files]\n",
    "        \n",
    "        #Class Name\n",
    "        self.class_name = [meta_data[file[-2][0] - 1][0] for file in files]\n",
    "        \n",
    "        #Get Car Year\n",
    "        self.carYear, self.carYear_ID = utils.get_Year(self.class_name)\n",
    "\n",
    "        #Get Car Maker\n",
    "        self.carMaker, self.carMaker_ID = utils.get_Maker(self.class_name)\n",
    "        \n",
    "        #Get Car Type\n",
    "        self.carType, self.carType_ID = utils.get_Type(self.class_name)\n",
    "        \n",
    "        #change and Move there is still time\n",
    "        self.year_count = len(np.unique(self.carYear_ID))\n",
    "        self.maker_count = len(np.unique(self.carMaker_ID))\n",
    "        self.type_count = len(np.unique(self.carType_ID))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        if self.image_transform:\n",
    "            img = self.image_transform(img)\n",
    "        \n",
    "        target = torch.from_numpy(np.array(self.id[idx]))[0]\n",
    "        \n",
    "        class_count = {\n",
    "                      \"year\": self.year_count,\n",
    "                      \"maker\": self.maker_count,\n",
    "                      \"type\": self.type_count\n",
    "                     }\n",
    "\n",
    "        sample = {'Image':img, 'class_ID':target, \"class_name\":self.class_name[idx],\n",
    "                 'year_ID':self.carYear_ID[idx], 'maker_ID':self.carMaker_ID[idx],\n",
    "                 'type_ID':self.carType_ID[idx], \"class_count\":class_count}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset creating\n",
    "        a weight depeding of the frequency of the class\n",
    "    Arguments:\n",
    "        dataset (list, optional): a list of indices\n",
    "        class_type (int, optional): \n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_type):\n",
    "                      \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        \n",
    "        self.num_samples = len(self.indices) \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx, class_type)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx, class_type)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx, class_type):\n",
    "        return dataset[idx][class_type].item()\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "                self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"data/cars_train/\"\n",
    "car_annotations_path = \"data/devkit/cars_train_annos.mat\"\n",
    "car_metadata_path = \"data/devkit/cars_meta.mat\"\n",
    "\n",
    "#Load Meta Data\n",
    "meta_data = loadmat(car_metadata_path)\n",
    "meta_data = np.concatenate(meta_data[\"class_names\"][0])\n",
    "\n",
    "#nb_classes\n",
    "nb_classes = len(meta_data)\n",
    "\n",
    "#Load and split train, val and test samples\n",
    "dataset = utils.Load_Images(root_dir=root_dir, annotations_path=car_annotations_path, seed=seed, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transformers = {'training': transforms.Compose([transforms.Resize(230),\n",
    "                                                      transforms.CenterCrop(224),\n",
    "                                                     transforms.RandomHorizontalFlip(0.8),\n",
    "                                                     transforms.ColorJitter(brightness=0.8, contrast=0.8),\n",
    "                                                     transforms.ToTensor()]),\n",
    "                      'validation': transforms.Compose([transforms.Resize(230),\n",
    "                                                      transforms.CenterCrop(224),\n",
    "                                                       transforms.ToTensor()])\n",
    "                     }\n",
    "    \n",
    "\n",
    "training_data = car_dataset(dataset[\"training\"],\n",
    "                            root_dir = root_dir,\n",
    "                            meta_data = meta_data,\n",
    "                            image_transform = image_transformers[\"training\"]\n",
    "                           )\n",
    "\n",
    "#\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=15, \n",
    "                                           sampler=ImbalancedDatasetSampler(training_data, \"maker_ID\"))\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(training_data, batch_size=15, shuffle=True)\n",
    "\n",
    "validation_data = car_dataset(dataset[\"validation\"], \n",
    "                             root_dir = root_dir,\n",
    "                             meta_data = meta_data,\n",
    "                             image_transform  = image_transformers[\"validation\"])\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, \n",
    "                                                batch_size=15, shuffle=True)\n",
    "\n",
    "'''\n",
    "testing_data = car_dataset(dataset[\"test\"], \n",
    "                           root_dir = root_dir,\n",
    "                           meta_data = meta_data,\n",
    "                           image_transform  = image_transformers[\"validation\"])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testing_data, batch_size=15, shuffle=True)\n",
    "'''\n",
    "\n",
    "dataloaders = {\"training\":train_loader, \"validation\":validation_loader}\n",
    "dataSizes = {\"training\":len(dataset[\"training\"]), \"validation\":len(dataset[\"validation\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Count per each Class\n",
    "class_year = training_data[0][\"class_count\"][\"year\"]\n",
    "class_maker = training_data[0][\"class_count\"][\"maker\"]\n",
    "class_type = training_data[0][\"class_count\"][\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Load Pretrained Model\n",
    "fr_model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "#freeze layers\n",
    "for param in fr_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "print(fr_model)\n",
    "num_ftrs = fr_model.fc.in_features\n",
    "fr_model.fc = torch.nn.Linear(num_ftrs, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class multi_task_model(torch.nn.Module):\n",
    "    def __init__(self, base_model,nb_maker_classes, nb_type_classes):\n",
    "        super(multi_task_model, self).__init__()\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        \n",
    "        self.x1 = torch.nn.Linear(1024, 512)\n",
    "        torch.nn.init.xavier_normal_(self.x1.weight) \n",
    "        self.bn1 = torch.nn.BatchNorm1d(512, eps = 2e-1)\n",
    "        \n",
    "        \n",
    "        #could overfit add, dropout??\n",
    "        self.x2 = torch.nn.Linear(512,512)\n",
    "        torch.nn.init.xavier_normal_(self.x2.weight)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(512, eps = 2e-1)\n",
    "        \n",
    "        #adding one more on top of the heads\n",
    "        self.x3 = torch.nn.Linear(512, 256)\n",
    "        torch.nn.init.xavier_normal_(self.x3.weight)\n",
    "        \n",
    "        #Connect the base Model to each head\n",
    "        self.y_maker = torch.nn.Linear(256, nb_maker_classes)\n",
    "        torch.nn.init.xavier_normal_(self.y_maker.weight)\n",
    "        self.y_type = torch.nn.Linear(256, nb_type_classes)\n",
    "        torch.nn.init.xavier_normal_(self.y_type.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.base_model(x)\n",
    "        \n",
    "        x1 = self.bn1(F.relu(self.x1(x1)))\n",
    "        x1 = self.bn2(F.relu(self.x2(x1)))\n",
    "        \n",
    "        y_maker_output = F.softmax(self.y_maker(self.x3(x1)))\n",
    "        y_type_output = F.softmax(self.y_type(self.x3(x1)))\n",
    "        \n",
    "        return y_maker_output, y_type_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_task_model(\n",
      "  (base_model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  )\n",
      "  (x1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=0.2, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (x2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=0.2, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (x3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (y_maker): Linear(in_features=256, out_features=49, bias=True)\n",
      "  (y_type): Linear(in_features=256, out_features=12, bias=True)\n",
      ")\n",
      "<generator object Module.parameters at 0x00000160A79BFFC0>\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "MTL_model = multi_task_model(fr_model, class_maker, class_type)\n",
    "MTL_model = MTL_model.to(device)\n",
    "\n",
    "print(MTL_model)\n",
    "print(MTL_model.parameters())   \n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    #last recorded best loss\n",
    "    best_acc = 150\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 30)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['training', 'validation']:\n",
    "            if phase == 'training':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            year_loss = 0.0\n",
    "            type_loss = 0.0\n",
    "            maker_loss = 0.0\n",
    "\n",
    "            correct_year = 0.0\n",
    "            correct_maker = 0.0\n",
    "            correct_type = 0.0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for batch_idx, data in enumerate(dataloaders[phase]):\n",
    "                \n",
    "                inputs = data[\"Image\"].to(device)\n",
    "                \n",
    "                maker_labels = data[\"maker_ID\"].to(device)\n",
    "                type_labels = data[\"type_ID\"].to(device)\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                #set to true when training\n",
    "                with torch.set_grad_enabled(phase == 'training'):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    #Order: maker, type\n",
    "                    loss_maker = criterion(outputs[0], maker_labels.long())\n",
    "                    loss_type = criterion(outputs[1], type_labels.long())\n",
    "                    \n",
    "                    if phase == 'training':\n",
    "                        \n",
    "                        #take sum of all loses\n",
    "                        total_loss = loss_maker + loss_type\n",
    "                        \n",
    "                        total_loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += total_loss.item() * inputs.size(0)\n",
    "                \n",
    "                type_loss += loss_maker.item() * inputs.size(0)\n",
    "                maker_loss += loss_type.item() * inputs.size(0)    \n",
    "                \n",
    "                correct_counts_maker = torch.max(outputs[0], dim=1)[1].eq(maker_labels.data.view_as(torch.max(outputs[0], dim=1)[1]))\n",
    "                correct_counts_type = torch.max(outputs[1], dim=1)[1].eq(type_labels.data.view_as(torch.max(outputs[1], dim=1)[1]))\n",
    "                \n",
    "                acc_m = torch.mean(correct_counts_maker.type(torch.FloatTensor))\n",
    "                acc_t = torch.mean(correct_counts_type.type(torch.FloatTensor))\n",
    "                \n",
    "                correct_maker += acc_m.item() * inputs.size(0)\n",
    "                correct_type += acc_t.item() * inputs.size(0)\n",
    "                \n",
    "\n",
    "                      \n",
    "            step_loss = running_loss / dataSizes[phase]\n",
    "            \n",
    "            maker_loss_total = maker_loss / dataSizes[phase]\n",
    "            type_loss_total = type_loss / dataSizes[phase]\n",
    "\n",
    "            maker_acc = correct_maker / dataSizes[phase]\n",
    "            type_acc = correct_type / dataSizes[phase]\n",
    "            \n",
    "            print('{} total loss: {:.4f}'.format(phase, step_loss))\n",
    "            print('{} Maker_acc: {:.2f}  Type_acc: {:.2f}'.format(phase, maker_acc, type_acc))\n",
    "            print('{} Maker_loss: {:.4f}  Type_loss: {:.4f}'.format(phase,maker_loss_total,type_loss_total))\n",
    "            history.append([epoch, step_loss, maker_loss_total, type_loss_total, maker_acc, type_acc])\n",
    "            \n",
    "            # deep copy the model depending on best accuracy of Car Maker\n",
    "            \n",
    "            if phase == 'validation' and step_loss < best_acc:\n",
    "                print('saving with loss of {}'.format(step_loss), 'improved over previous {}'.format(best_acc))\n",
    "                best_acc = step_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(float(best_acc)))\n",
    "\n",
    "    # load best model weights\n",
    "    last_model_weights = copy.deepcopy(model.state_dict())\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, last_model_weights, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lrlast = .002 #learning rate for heads\n",
    "lrmain = .0005\n",
    "\n",
    "optimizers = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\":MTL_model.base_model.parameters(),\"lr\": lrmain},\n",
    "        {\"params\":MTL_model.x1.parameters(), \"lr\": lrlast},\n",
    "        {\"params\":MTL_model.x2.parameters(), \"lr\": lrlast},\n",
    "        {\"params\":MTL_model.y_maker.parameters(), \"lr\": lrlast},\n",
    "        {\"params\":MTL_model.y_type.parameters(), \"lr\": lrlast}  \n",
    "   ])\n",
    "\n",
    "#optim1 = optim.Adam(model_1.parameters(), lr=0.0001)#,momentum=.9)\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optimizers\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryan_\\.conda\\envs\\azure_grabchallenge\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\ryan_\\.conda\\envs\\azure_grabchallenge\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training total loss: 6.0648\n",
      "training Maker_acc: 0.08  Type_acc: 0.40\n",
      "training Maker_loss: 2.2223  Type_loss: 3.8425\n",
      "validation total loss: 5.9958\n",
      "validation Maker_acc: 0.03  Type_acc: 0.39\n",
      "validation Maker_loss: 2.2309  Type_loss: 3.8914\n",
      "saving with loss of 5.995805740356445 improved over previous 150\n",
      "Epoch 1/29\n",
      "------------------------------\n",
      "training total loss: 5.9752\n",
      "training Maker_acc: 0.12  Type_acc: 0.44\n",
      "training Maker_loss: 2.1707  Type_loss: 3.8045\n",
      "validation total loss: 6.1490\n",
      "validation Maker_acc: 0.03  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2079  Type_loss: 3.8883\n",
      "Epoch 2/29\n",
      "------------------------------\n",
      "training total loss: 5.9665\n",
      "training Maker_acc: 0.12  Type_acc: 0.45\n",
      "training Maker_loss: 2.1620  Type_loss: 3.8046\n",
      "validation total loss: 6.3144\n",
      "validation Maker_acc: 0.04  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2061  Type_loss: 3.8870\n",
      "Epoch 3/29\n",
      "------------------------------\n",
      "training total loss: 5.9379\n",
      "training Maker_acc: 0.14  Type_acc: 0.47\n",
      "training Maker_loss: 2.1516  Type_loss: 3.7864\n",
      "validation total loss: 5.8322\n",
      "validation Maker_acc: 0.05  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1901  Type_loss: 3.8695\n",
      "saving with loss of 5.832231521606445 improved over previous 5.995805740356445\n",
      "Epoch 4/29\n",
      "------------------------------\n",
      "training total loss: 5.9022\n",
      "training Maker_acc: 0.16  Type_acc: 0.48\n",
      "training Maker_loss: 2.1352  Type_loss: 3.7670\n",
      "validation total loss: 5.5429\n",
      "validation Maker_acc: 0.06  Type_acc: 0.43\n",
      "validation Maker_loss: 2.1844  Type_loss: 3.8670\n",
      "saving with loss of 5.542946815490723 improved over previous 5.832231521606445\n",
      "Epoch 5/29\n",
      "------------------------------\n",
      "training total loss: 5.8790\n",
      "training Maker_acc: 0.17  Type_acc: 0.49\n",
      "training Maker_loss: 2.1216  Type_loss: 3.7574\n",
      "validation total loss: 6.1395\n",
      "validation Maker_acc: 0.06  Type_acc: 0.43\n",
      "validation Maker_loss: 2.1823  Type_loss: 3.8656\n",
      "Epoch 6/29\n",
      "------------------------------\n",
      "training total loss: 5.8646\n",
      "training Maker_acc: 0.18  Type_acc: 0.50\n",
      "training Maker_loss: 2.1153  Type_loss: 3.7493\n",
      "validation total loss: 6.4619\n",
      "validation Maker_acc: 0.06  Type_acc: 0.45\n",
      "validation Maker_loss: 2.1668  Type_loss: 3.8643\n",
      "Epoch 7/29\n",
      "------------------------------\n",
      "training total loss: 5.8502\n",
      "training Maker_acc: 0.18  Type_acc: 0.51\n",
      "training Maker_loss: 2.1044  Type_loss: 3.7458\n",
      "validation total loss: 5.7360\n",
      "validation Maker_acc: 0.06  Type_acc: 0.44\n",
      "validation Maker_loss: 2.1702  Type_loss: 3.8631\n",
      "Epoch 8/29\n",
      "------------------------------\n",
      "training total loss: 5.8449\n",
      "training Maker_acc: 0.19  Type_acc: 0.51\n",
      "training Maker_loss: 2.1112  Type_loss: 3.7336\n",
      "validation total loss: 5.7564\n",
      "validation Maker_acc: 0.06  Type_acc: 0.45\n",
      "validation Maker_loss: 2.1647  Type_loss: 3.8622\n",
      "Epoch 9/29\n",
      "------------------------------\n",
      "training total loss: 5.8490\n",
      "training Maker_acc: 0.20  Type_acc: 0.50\n",
      "training Maker_loss: 2.1168  Type_loss: 3.7321\n",
      "validation total loss: 6.1128\n",
      "validation Maker_acc: 0.06  Type_acc: 0.45\n",
      "validation Maker_loss: 2.1691  Type_loss: 3.8617\n",
      "Epoch 10/29\n",
      "------------------------------\n",
      "training total loss: 5.8400\n",
      "training Maker_acc: 0.19  Type_acc: 0.51\n",
      "training Maker_loss: 2.1059  Type_loss: 3.7341\n",
      "validation total loss: 5.9854\n",
      "validation Maker_acc: 0.06  Type_acc: 0.45\n",
      "validation Maker_loss: 2.1695  Type_loss: 3.8618\n",
      "Epoch 11/29\n",
      "------------------------------\n",
      "training total loss: 5.8288\n",
      "training Maker_acc: 0.20  Type_acc: 0.51\n",
      "training Maker_loss: 2.1029  Type_loss: 3.7259\n",
      "validation total loss: 5.7602\n",
      "validation Maker_acc: 0.06  Type_acc: 0.45\n",
      "validation Maker_loss: 2.1655  Type_loss: 3.8626\n",
      "Epoch 12/29\n",
      "------------------------------\n",
      "training total loss: 5.8483\n",
      "training Maker_acc: 0.19  Type_acc: 0.50\n",
      "training Maker_loss: 2.1156  Type_loss: 3.7327\n",
      "validation total loss: 5.3002\n",
      "validation Maker_acc: 0.06  Type_acc: 0.46\n",
      "validation Maker_loss: 2.1605  Type_loss: 3.8615\n",
      "saving with loss of 5.300154209136963 improved over previous 5.542946815490723\n",
      "Epoch 13/29\n",
      "------------------------------\n",
      "training total loss: 5.8487\n",
      "training Maker_acc: 0.19  Type_acc: 0.51\n",
      "training Maker_loss: 2.1084  Type_loss: 3.7403\n",
      "validation total loss: 5.8287\n",
      "validation Maker_acc: 0.06  Type_acc: 0.45\n",
      "validation Maker_loss: 2.1660  Type_loss: 3.8626\n",
      "Epoch 14/29\n",
      "------------------------------\n",
      "training total loss: 5.8478\n",
      "training Maker_acc: 0.19  Type_acc: 0.50\n",
      "training Maker_loss: 2.1143  Type_loss: 3.7334\n",
      "validation total loss: 5.9594\n",
      "validation Maker_acc: 0.06  Type_acc: 0.46\n",
      "validation Maker_loss: 2.1612  Type_loss: 3.8616\n",
      "Epoch 15/29\n",
      "------------------------------\n",
      "training total loss: 5.8463\n",
      "training Maker_acc: 0.20  Type_acc: 0.50\n",
      "training Maker_loss: 2.1172  Type_loss: 3.7291\n",
      "validation total loss: 5.9838\n",
      "validation Maker_acc: 0.06  Type_acc: 0.46\n",
      "validation Maker_loss: 2.1632  Type_loss: 3.8624\n",
      "Epoch 16/29\n",
      "------------------------------\n",
      "training total loss: 5.8393\n",
      "training Maker_acc: 0.19  Type_acc: 0.51\n",
      "training Maker_loss: 2.1060  Type_loss: 3.7333\n",
      "validation total loss: 5.2039\n",
      "validation Maker_acc: 0.06  Type_acc: 0.45\n",
      "validation Maker_loss: 2.1601  Type_loss: 3.8648\n",
      "saving with loss of 5.203857421875 improved over previous 5.300154209136963\n",
      "Epoch 17/29\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#my last\n",
    "trained_model, l_model, history = train_model(MTL_model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryan_\\.conda\\envs\\azure_grabchallenge\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\ryan_\\.conda\\envs\\azure_grabchallenge\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training total loss: 5.8508\n",
      "training Maker_acc: 0.28  Type_acc: 0.41\n",
      "training Maker_loss: 2.2049  Type_loss: 3.6459\n",
      "validation total loss: 5.7882\n",
      "validation Maker_acc: 0.08  Type_acc: 0.27\n",
      "validation Maker_loss: 2.3422  Type_loss: 3.8446\n",
      "saving with loss of 5.788242816925049 improved over previous 150\n",
      "Epoch 1/29\n",
      "------------------------------\n",
      "training total loss: 5.6737\n",
      "training Maker_acc: 0.36  Type_acc: 0.50\n",
      "training Maker_loss: 2.1113  Type_loss: 3.5624\n",
      "validation total loss: 5.6764\n",
      "validation Maker_acc: 0.09  Type_acc: 0.28\n",
      "validation Maker_loss: 2.3366  Type_loss: 3.8387\n",
      "saving with loss of 5.676398754119873 improved over previous 5.788242816925049\n",
      "Epoch 2/29\n",
      "------------------------------\n",
      "training total loss: 5.6232\n",
      "training Maker_acc: 0.39  Type_acc: 0.53\n",
      "training Maker_loss: 2.0824  Type_loss: 3.5408\n",
      "validation total loss: 5.8670\n",
      "validation Maker_acc: 0.09  Type_acc: 0.34\n",
      "validation Maker_loss: 2.2773  Type_loss: 3.8387\n",
      "Epoch 3/29\n",
      "------------------------------\n",
      "training total loss: 5.5926\n",
      "training Maker_acc: 0.39  Type_acc: 0.56\n",
      "training Maker_loss: 2.0605  Type_loss: 3.5321\n",
      "validation total loss: 5.7391\n",
      "validation Maker_acc: 0.10  Type_acc: 0.37\n",
      "validation Maker_loss: 2.2477  Type_loss: 3.8240\n",
      "Epoch 4/29\n",
      "------------------------------\n",
      "training total loss: 5.5566\n",
      "training Maker_acc: 0.42  Type_acc: 0.57\n",
      "training Maker_loss: 2.0478  Type_loss: 3.5088\n",
      "validation total loss: 5.1397\n",
      "validation Maker_acc: 0.11  Type_acc: 0.37\n",
      "validation Maker_loss: 2.2482  Type_loss: 3.8169\n",
      "saving with loss of 5.139708995819092 improved over previous 5.676398754119873\n",
      "Epoch 5/29\n",
      "------------------------------\n",
      "training total loss: 5.5148\n",
      "training Maker_acc: 0.43  Type_acc: 0.59\n",
      "training Maker_loss: 2.0234  Type_loss: 3.4914\n",
      "validation total loss: 5.7640\n",
      "validation Maker_acc: 0.11  Type_acc: 0.37\n",
      "validation Maker_loss: 2.2426  Type_loss: 3.8166\n",
      "Epoch 6/29\n",
      "------------------------------\n",
      "training total loss: 5.5187\n",
      "training Maker_acc: 0.43  Type_acc: 0.59\n",
      "training Maker_loss: 2.0235  Type_loss: 3.4953\n",
      "validation total loss: 5.6641\n",
      "validation Maker_acc: 0.11  Type_acc: 0.40\n",
      "validation Maker_loss: 2.2175  Type_loss: 3.8121\n",
      "Epoch 7/29\n",
      "------------------------------\n",
      "training total loss: 5.5176\n",
      "training Maker_acc: 0.43  Type_acc: 0.59\n",
      "training Maker_loss: 2.0217  Type_loss: 3.4959\n",
      "validation total loss: 5.5438\n",
      "validation Maker_acc: 0.11  Type_acc: 0.39\n",
      "validation Maker_loss: 2.2233  Type_loss: 3.8171\n",
      "Epoch 8/29\n",
      "------------------------------\n",
      "training total loss: 5.5057\n",
      "training Maker_acc: 0.43  Type_acc: 0.61\n",
      "training Maker_loss: 2.0082  Type_loss: 3.4975\n",
      "validation total loss: 5.1455\n",
      "validation Maker_acc: 0.11  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1940  Type_loss: 3.8129\n",
      "Epoch 9/29\n",
      "------------------------------\n",
      "training total loss: 5.4755\n",
      "training Maker_acc: 0.45  Type_acc: 0.62\n",
      "training Maker_loss: 1.9985  Type_loss: 3.4770\n",
      "validation total loss: 5.1450\n",
      "validation Maker_acc: 0.11  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2152  Type_loss: 3.8124\n",
      "Epoch 10/29\n",
      "------------------------------\n",
      "training total loss: 5.4745\n",
      "training Maker_acc: 0.45  Type_acc: 0.62\n",
      "training Maker_loss: 1.9962  Type_loss: 3.4784\n",
      "validation total loss: 5.1608\n",
      "validation Maker_acc: 0.12  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1945  Type_loss: 3.8116\n",
      "Epoch 11/29\n",
      "------------------------------\n",
      "training total loss: 5.4811\n",
      "training Maker_acc: 0.45  Type_acc: 0.62\n",
      "training Maker_loss: 2.0000  Type_loss: 3.4811\n",
      "validation total loss: 4.9556\n",
      "validation Maker_acc: 0.11  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2084  Type_loss: 3.8138\n",
      "saving with loss of 4.955569267272949 improved over previous 5.139708995819092\n",
      "Epoch 12/29\n",
      "------------------------------\n",
      "training total loss: 5.4536\n",
      "training Maker_acc: 0.46  Type_acc: 0.62\n",
      "training Maker_loss: 1.9924  Type_loss: 3.4612\n",
      "validation total loss: 5.5297\n",
      "validation Maker_acc: 0.12  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1921  Type_loss: 3.8102\n",
      "Epoch 13/29\n",
      "------------------------------\n",
      "training total loss: 5.4576\n",
      "training Maker_acc: 0.46  Type_acc: 0.63\n",
      "training Maker_loss: 1.9897  Type_loss: 3.4679\n",
      "validation total loss: 5.5178\n",
      "validation Maker_acc: 0.12  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1959  Type_loss: 3.8098\n",
      "Epoch 14/29\n",
      "------------------------------\n",
      "training total loss: 5.4843\n",
      "training Maker_acc: 0.44  Type_acc: 0.62\n",
      "training Maker_loss: 2.0005  Type_loss: 3.4838\n",
      "validation total loss: 5.3453\n",
      "validation Maker_acc: 0.12  Type_acc: 0.42\n",
      "validation Maker_loss: 2.2009  Type_loss: 3.8028\n",
      "Epoch 15/29\n",
      "------------------------------\n",
      "training total loss: 5.4525\n",
      "training Maker_acc: 0.47  Type_acc: 0.62\n",
      "training Maker_loss: 1.9933  Type_loss: 3.4592\n",
      "validation total loss: 5.9431\n",
      "validation Maker_acc: 0.11  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2064  Type_loss: 3.8108\n",
      "Epoch 16/29\n",
      "------------------------------\n",
      "training total loss: 5.4623\n",
      "training Maker_acc: 0.46  Type_acc: 0.63\n",
      "training Maker_loss: 1.9924  Type_loss: 3.4698\n",
      "validation total loss: 5.5483\n",
      "validation Maker_acc: 0.12  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2004  Type_loss: 3.8081\n",
      "Epoch 17/29\n",
      "------------------------------\n",
      "training total loss: 5.4637\n",
      "training Maker_acc: 0.46  Type_acc: 0.62\n",
      "training Maker_loss: 1.9966  Type_loss: 3.4671\n",
      "validation total loss: 6.3357\n",
      "validation Maker_acc: 0.11  Type_acc: 0.40\n",
      "validation Maker_loss: 2.2110  Type_loss: 3.8132\n",
      "Epoch 18/29\n",
      "------------------------------\n",
      "training total loss: 5.4616\n",
      "training Maker_acc: 0.46  Type_acc: 0.63\n",
      "training Maker_loss: 1.9926  Type_loss: 3.4689\n",
      "validation total loss: 5.7237\n",
      "validation Maker_acc: 0.12  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1910  Type_loss: 3.8052\n",
      "Epoch 19/29\n",
      "------------------------------\n",
      "training total loss: 5.4754\n",
      "training Maker_acc: 0.45  Type_acc: 0.62\n",
      "training Maker_loss: 1.9991  Type_loss: 3.4763\n",
      "validation total loss: 5.9528\n",
      "validation Maker_acc: 0.11  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2079  Type_loss: 3.8099\n",
      "Epoch 20/29\n",
      "------------------------------\n",
      "training total loss: 5.4667\n",
      "training Maker_acc: 0.45  Type_acc: 0.62\n",
      "training Maker_loss: 1.9951  Type_loss: 3.4717\n",
      "validation total loss: 6.1232\n",
      "validation Maker_acc: 0.11  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1970  Type_loss: 3.8105\n",
      "Epoch 21/29\n",
      "------------------------------\n",
      "training total loss: 5.4695\n",
      "training Maker_acc: 0.45  Type_acc: 0.62\n",
      "training Maker_loss: 1.9965  Type_loss: 3.4730\n",
      "validation total loss: 5.3315\n",
      "validation Maker_acc: 0.12  Type_acc: 0.43\n",
      "validation Maker_loss: 2.1881  Type_loss: 3.8104\n",
      "Epoch 22/29\n",
      "------------------------------\n",
      "training total loss: 5.4906\n",
      "training Maker_acc: 0.44  Type_acc: 0.61\n",
      "training Maker_loss: 2.0057  Type_loss: 3.4849\n",
      "validation total loss: 5.3451\n",
      "validation Maker_acc: 0.11  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2033  Type_loss: 3.8118\n",
      "Epoch 23/29\n",
      "------------------------------\n",
      "training total loss: 5.4745\n",
      "training Maker_acc: 0.45  Type_acc: 0.62\n",
      "training Maker_loss: 2.0004  Type_loss: 3.4742\n",
      "validation total loss: 5.2899\n",
      "validation Maker_acc: 0.12  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1986  Type_loss: 3.8102\n",
      "Epoch 24/29\n",
      "------------------------------\n",
      "training total loss: 5.4584\n",
      "training Maker_acc: 0.45  Type_acc: 0.63\n",
      "training Maker_loss: 1.9869  Type_loss: 3.4715\n",
      "validation total loss: 6.5433\n",
      "validation Maker_acc: 0.12  Type_acc: 0.43\n",
      "validation Maker_loss: 2.1881  Type_loss: 3.8077\n",
      "Epoch 25/29\n",
      "------------------------------\n",
      "training total loss: 5.4608\n",
      "training Maker_acc: 0.46  Type_acc: 0.63\n",
      "training Maker_loss: 1.9901  Type_loss: 3.4706\n",
      "validation total loss: 5.5289\n",
      "validation Maker_acc: 0.12  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2054  Type_loss: 3.8058\n",
      "Epoch 26/29\n",
      "------------------------------\n",
      "training total loss: 5.4476\n",
      "training Maker_acc: 0.47  Type_acc: 0.63\n",
      "training Maker_loss: 1.9899  Type_loss: 3.4577\n",
      "validation total loss: 5.5738\n",
      "validation Maker_acc: 0.12  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1919  Type_loss: 3.8109\n",
      "Epoch 27/29\n",
      "------------------------------\n",
      "training total loss: 5.4719\n",
      "training Maker_acc: 0.45  Type_acc: 0.62\n",
      "training Maker_loss: 2.0018  Type_loss: 3.4701\n",
      "validation total loss: 6.1415\n",
      "validation Maker_acc: 0.11  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2095  Type_loss: 3.8131\n",
      "Epoch 28/29\n",
      "------------------------------\n",
      "training total loss: 5.4754\n",
      "training Maker_acc: 0.45  Type_acc: 0.61\n",
      "training Maker_loss: 2.0024  Type_loss: 3.4730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation total loss: 5.9631\n",
      "validation Maker_acc: 0.12  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2078  Type_loss: 3.8101\n",
      "Epoch 29/29\n",
      "------------------------------\n",
      "training total loss: 5.4933\n",
      "training Maker_acc: 0.44  Type_acc: 0.61\n",
      "training Maker_loss: 2.0083  Type_loss: 3.4850\n",
      "validation total loss: 5.6345\n",
      "validation Maker_acc: 0.11  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1993  Type_loss: 3.8103\n",
      "Training complete in 75m 23s\n",
      "Best val Acc: 4.955569\n"
     ]
    }
   ],
   "source": [
    "trained_model, history = train_model(MTL_model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/59\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryan_\\.conda\\envs\\azure_grabchallenge\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\ryan_\\.conda\\envs\\azure_grabchallenge\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training total loss: 5.7111\n",
      "training Maker_acc: 0.36  Type_acc: 0.50\n",
      "training Maker_loss: 2.1280  Type_loss: 3.5830\n",
      "validation total loss: 6.0422\n",
      "validation Maker_acc: 0.10  Type_acc: 0.32\n",
      "validation Maker_loss: 2.2889  Type_loss: 3.8244\n",
      "saving with loss of 6.04217529296875 improved over previous 150\n",
      "Epoch 1/59\n",
      "------------------------------\n",
      "training total loss: 5.5021\n",
      "training Maker_acc: 0.47  Type_acc: 0.58\n",
      "training Maker_loss: 2.0379  Type_loss: 3.4642\n",
      "validation total loss: 5.7549\n",
      "validation Maker_acc: 0.13  Type_acc: 0.40\n",
      "validation Maker_loss: 2.2195  Type_loss: 3.7958\n",
      "saving with loss of 5.7549028396606445 improved over previous 6.04217529296875\n",
      "Epoch 2/59\n",
      "------------------------------\n",
      "training total loss: 5.4268\n",
      "training Maker_acc: 0.51  Type_acc: 0.62\n",
      "training Maker_loss: 2.0039  Type_loss: 3.4229\n",
      "validation total loss: 5.6585\n",
      "validation Maker_acc: 0.13  Type_acc: 0.42\n",
      "validation Maker_loss: 2.1941  Type_loss: 3.7887\n",
      "saving with loss of 5.658489227294922 improved over previous 5.7549028396606445\n",
      "Epoch 3/59\n",
      "------------------------------\n",
      "training total loss: 5.4063\n",
      "training Maker_acc: 0.52  Type_acc: 0.63\n",
      "training Maker_loss: 1.9928  Type_loss: 3.4135\n",
      "validation total loss: 5.8403\n",
      "validation Maker_acc: 0.14  Type_acc: 0.41\n",
      "validation Maker_loss: 2.2006  Type_loss: 3.7862\n",
      "Epoch 4/59\n",
      "------------------------------\n",
      "training total loss: 5.3610\n",
      "training Maker_acc: 0.54  Type_acc: 0.65\n",
      "training Maker_loss: 1.9686  Type_loss: 3.3924\n",
      "validation total loss: 5.1381\n",
      "validation Maker_acc: 0.15  Type_acc: 0.46\n",
      "validation Maker_loss: 2.1589  Type_loss: 3.7782\n",
      "saving with loss of 5.138114929199219 improved over previous 5.658489227294922\n",
      "Epoch 5/59\n",
      "------------------------------\n",
      "training total loss: 5.3090\n",
      "training Maker_acc: 0.57  Type_acc: 0.68\n",
      "training Maker_loss: 1.9443  Type_loss: 3.3647\n",
      "validation total loss: 5.8672\n",
      "validation Maker_acc: 0.15  Type_acc: 0.46\n",
      "validation Maker_loss: 2.1559  Type_loss: 3.7785\n",
      "Epoch 6/59\n",
      "------------------------------\n",
      "training total loss: 5.2992\n",
      "training Maker_acc: 0.57  Type_acc: 0.69\n",
      "training Maker_loss: 1.9363  Type_loss: 3.3629\n",
      "validation total loss: 5.7841\n",
      "validation Maker_acc: 0.15  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1351  Type_loss: 3.7773\n",
      "Epoch 7/59\n",
      "------------------------------\n",
      "training total loss: 5.3112\n",
      "training Maker_acc: 0.56  Type_acc: 0.68\n",
      "training Maker_loss: 1.9387  Type_loss: 3.3725\n",
      "validation total loss: 5.8268\n",
      "validation Maker_acc: 0.15  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1305  Type_loss: 3.7733\n",
      "Epoch 8/59\n",
      "------------------------------\n",
      "training total loss: 5.2900\n",
      "training Maker_acc: 0.57  Type_acc: 0.69\n",
      "training Maker_loss: 1.9324  Type_loss: 3.3576\n",
      "validation total loss: 5.1436\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1340  Type_loss: 3.7656\n",
      "Epoch 9/59\n",
      "------------------------------\n",
      "training total loss: 5.2748\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9243  Type_loss: 3.3505\n",
      "validation total loss: 4.8849\n",
      "validation Maker_acc: 0.16  Type_acc: 0.47\n",
      "validation Maker_loss: 2.1477  Type_loss: 3.7663\n",
      "saving with loss of 4.884886264801025 improved over previous 5.138114929199219\n",
      "Epoch 10/59\n",
      "------------------------------\n",
      "training total loss: 5.2748\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9252  Type_loss: 3.3496\n",
      "validation total loss: 5.1442\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1336  Type_loss: 3.7632\n",
      "Epoch 11/59\n",
      "------------------------------\n",
      "training total loss: 5.2861\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9279  Type_loss: 3.3582\n",
      "validation total loss: 4.5816\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1378  Type_loss: 3.7662\n",
      "saving with loss of 4.581578731536865 improved over previous 4.884886264801025\n",
      "Epoch 12/59\n",
      "------------------------------\n",
      "training total loss: 5.2529\n",
      "training Maker_acc: 0.60  Type_acc: 0.70\n",
      "training Maker_loss: 1.9175  Type_loss: 3.3354\n",
      "validation total loss: 5.5465\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1345  Type_loss: 3.7643\n",
      "Epoch 13/59\n",
      "------------------------------\n",
      "training total loss: 5.2604\n",
      "training Maker_acc: 0.59  Type_acc: 0.71\n",
      "training Maker_loss: 1.9120  Type_loss: 3.3484\n",
      "validation total loss: 5.2891\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1345  Type_loss: 3.7654\n",
      "Epoch 14/59\n",
      "------------------------------\n",
      "training total loss: 5.2682\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9213  Type_loss: 3.3468\n",
      "validation total loss: 5.1577\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1356  Type_loss: 3.7634\n",
      "Epoch 15/59\n",
      "------------------------------\n",
      "training total loss: 5.2617\n",
      "training Maker_acc: 0.59  Type_acc: 0.71\n",
      "training Maker_loss: 1.9191  Type_loss: 3.3426\n",
      "validation total loss: 5.9136\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1403  Type_loss: 3.7663\n",
      "Epoch 16/59\n",
      "------------------------------\n",
      "training total loss: 5.2572\n",
      "training Maker_acc: 0.59  Type_acc: 0.71\n",
      "training Maker_loss: 1.9123  Type_loss: 3.3449\n",
      "validation total loss: 5.9563\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1289  Type_loss: 3.7626\n",
      "Epoch 17/59\n",
      "------------------------------\n",
      "training total loss: 5.2663\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9176  Type_loss: 3.3487\n",
      "validation total loss: 5.9528\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1387  Type_loss: 3.7653\n",
      "Epoch 18/59\n",
      "------------------------------\n",
      "training total loss: 5.2600\n",
      "training Maker_acc: 0.59  Type_acc: 0.71\n",
      "training Maker_loss: 1.9119  Type_loss: 3.3481\n",
      "validation total loss: 4.9651\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1255  Type_loss: 3.7632\n",
      "Epoch 19/59\n",
      "------------------------------\n",
      "training total loss: 5.2717\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9233  Type_loss: 3.3485\n",
      "validation total loss: 5.5404\n",
      "validation Maker_acc: 0.16  Type_acc: 0.47\n",
      "validation Maker_loss: 2.1513  Type_loss: 3.7685\n",
      "Epoch 20/59\n",
      "------------------------------\n",
      "training total loss: 5.2740\n",
      "training Maker_acc: 0.58  Type_acc: 0.69\n",
      "training Maker_loss: 1.9261  Type_loss: 3.3478\n",
      "validation total loss: 5.9165\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1380  Type_loss: 3.7635\n",
      "Epoch 21/59\n",
      "------------------------------\n",
      "training total loss: 5.2704\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9210  Type_loss: 3.3494\n",
      "validation total loss: 4.8138\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1282  Type_loss: 3.7633\n",
      "Epoch 22/59\n",
      "------------------------------\n",
      "training total loss: 5.2721\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9231  Type_loss: 3.3490\n",
      "validation total loss: 5.1648\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1391  Type_loss: 3.7665\n",
      "Epoch 23/59\n",
      "------------------------------\n",
      "training total loss: 5.2851\n",
      "training Maker_acc: 0.58  Type_acc: 0.69\n",
      "training Maker_loss: 1.9291  Type_loss: 3.3560\n",
      "validation total loss: 5.5059\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1373  Type_loss: 3.7640\n",
      "Epoch 24/59\n",
      "------------------------------\n",
      "training total loss: 5.2634\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9198  Type_loss: 3.3436\n",
      "validation total loss: 5.7554\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1369  Type_loss: 3.7645\n",
      "Epoch 25/59\n",
      "------------------------------\n",
      "training total loss: 5.2561\n",
      "training Maker_acc: 0.59  Type_acc: 0.71\n",
      "training Maker_loss: 1.9116  Type_loss: 3.3445\n",
      "validation total loss: 5.6360\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1397  Type_loss: 3.7638\n",
      "Epoch 26/59\n",
      "------------------------------\n",
      "training total loss: 5.2532\n",
      "training Maker_acc: 0.60  Type_acc: 0.71\n",
      "training Maker_loss: 1.9157  Type_loss: 3.3375\n",
      "validation total loss: 5.5405\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1308  Type_loss: 3.7651\n",
      "Epoch 27/59\n",
      "------------------------------\n",
      "training total loss: 5.2772\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9241  Type_loss: 3.3531\n",
      "validation total loss: 5.5656\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1436  Type_loss: 3.7649\n",
      "Epoch 28/59\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training total loss: 5.2780\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9201  Type_loss: 3.3579\n",
      "validation total loss: 6.1526\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1416  Type_loss: 3.7657\n",
      "Epoch 29/59\n",
      "------------------------------\n",
      "training total loss: 5.2704\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9194  Type_loss: 3.3509\n",
      "validation total loss: 5.6541\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1301  Type_loss: 3.7640\n",
      "Epoch 30/59\n",
      "------------------------------\n",
      "training total loss: 5.2566\n",
      "training Maker_acc: 0.59  Type_acc: 0.71\n",
      "training Maker_loss: 1.9154  Type_loss: 3.3411\n",
      "validation total loss: 5.4713\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1407  Type_loss: 3.7632\n",
      "Epoch 31/59\n",
      "------------------------------\n",
      "training total loss: 5.2643\n",
      "training Maker_acc: 0.59  Type_acc: 0.71\n",
      "training Maker_loss: 1.9165  Type_loss: 3.3478\n",
      "validation total loss: 5.1878\n",
      "validation Maker_acc: 0.16  Type_acc: 0.47\n",
      "validation Maker_loss: 2.1465  Type_loss: 3.7638\n",
      "Epoch 32/59\n",
      "------------------------------\n",
      "training total loss: 5.2662\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9237  Type_loss: 3.3426\n",
      "validation total loss: 4.6950\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1389  Type_loss: 3.7629\n",
      "Epoch 33/59\n",
      "------------------------------\n",
      "training total loss: 5.2707\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9259  Type_loss: 3.3448\n",
      "validation total loss: 5.5336\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1302  Type_loss: 3.7651\n",
      "Epoch 34/59\n",
      "------------------------------\n",
      "training total loss: 5.2757\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9267  Type_loss: 3.3490\n",
      "validation total loss: 5.5934\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1301  Type_loss: 3.7648\n",
      "Epoch 35/59\n",
      "------------------------------\n",
      "training total loss: 5.2559\n",
      "training Maker_acc: 0.59  Type_acc: 0.71\n",
      "training Maker_loss: 1.9144  Type_loss: 3.3415\n",
      "validation total loss: 5.5283\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1338  Type_loss: 3.7630\n",
      "Epoch 36/59\n",
      "------------------------------\n",
      "training total loss: 5.2739\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9233  Type_loss: 3.3506\n",
      "validation total loss: 4.9477\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1319  Type_loss: 3.7650\n",
      "Epoch 37/59\n",
      "------------------------------\n",
      "training total loss: 5.2650\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9178  Type_loss: 3.3472\n",
      "validation total loss: 5.9337\n",
      "validation Maker_acc: 0.15  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1286  Type_loss: 3.7662\n",
      "Epoch 38/59\n",
      "------------------------------\n",
      "training total loss: 5.2674\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9204  Type_loss: 3.3470\n",
      "validation total loss: 5.0177\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1281  Type_loss: 3.7656\n",
      "Epoch 39/59\n",
      "------------------------------\n",
      "training total loss: 5.2733\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9211  Type_loss: 3.3522\n",
      "validation total loss: 5.3938\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1362  Type_loss: 3.7639\n",
      "Epoch 40/59\n",
      "------------------------------\n",
      "training total loss: 5.2855\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9263  Type_loss: 3.3591\n",
      "validation total loss: 5.7395\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1398  Type_loss: 3.7644\n",
      "Epoch 41/59\n",
      "------------------------------\n",
      "training total loss: 5.2787\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9210  Type_loss: 3.3577\n",
      "validation total loss: 5.3415\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1325  Type_loss: 3.7623\n",
      "Epoch 42/59\n",
      "------------------------------\n",
      "training total loss: 5.2583\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9178  Type_loss: 3.3405\n",
      "validation total loss: 5.1912\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1468  Type_loss: 3.7657\n",
      "Epoch 43/59\n",
      "------------------------------\n",
      "training total loss: 5.2750\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9224  Type_loss: 3.3526\n",
      "validation total loss: 5.0875\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1336  Type_loss: 3.7635\n",
      "Epoch 44/59\n",
      "------------------------------\n",
      "training total loss: 5.2645\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9235  Type_loss: 3.3409\n",
      "validation total loss: 5.6729\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1342  Type_loss: 3.7638\n",
      "Epoch 45/59\n",
      "------------------------------\n",
      "training total loss: 5.2678\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9194  Type_loss: 3.3484\n",
      "validation total loss: 5.6228\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1356  Type_loss: 3.7660\n",
      "Epoch 46/59\n",
      "------------------------------\n",
      "training total loss: 5.2694\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9238  Type_loss: 3.3456\n",
      "validation total loss: 5.7091\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1386  Type_loss: 3.7658\n",
      "Epoch 47/59\n",
      "------------------------------\n",
      "training total loss: 5.2659\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9224  Type_loss: 3.3434\n",
      "validation total loss: 5.2917\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1305  Type_loss: 3.7640\n",
      "Epoch 48/59\n",
      "------------------------------\n",
      "training total loss: 5.2765\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9220  Type_loss: 3.3545\n",
      "validation total loss: 5.4732\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1338  Type_loss: 3.7654\n",
      "Epoch 49/59\n",
      "------------------------------\n",
      "training total loss: 5.2580\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9187  Type_loss: 3.3393\n",
      "validation total loss: 6.3274\n",
      "validation Maker_acc: 0.16  Type_acc: 0.47\n",
      "validation Maker_loss: 2.1464  Type_loss: 3.7655\n",
      "Epoch 50/59\n",
      "------------------------------\n",
      "training total loss: 5.2764\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9269  Type_loss: 3.3494\n",
      "validation total loss: 5.1420\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1383  Type_loss: 3.7633\n",
      "Epoch 51/59\n",
      "------------------------------\n",
      "training total loss: 5.2628\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9163  Type_loss: 3.3465\n",
      "validation total loss: 4.8616\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1309  Type_loss: 3.7637\n",
      "Epoch 52/59\n",
      "------------------------------\n",
      "training total loss: 5.2751\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9230  Type_loss: 3.3521\n",
      "validation total loss: 4.8148\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1274  Type_loss: 3.7653\n",
      "Epoch 53/59\n",
      "------------------------------\n",
      "training total loss: 5.2907\n",
      "training Maker_acc: 0.57  Type_acc: 0.69\n",
      "training Maker_loss: 1.9305  Type_loss: 3.3602\n",
      "validation total loss: 5.5917\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1344  Type_loss: 3.7651\n",
      "Epoch 54/59\n",
      "------------------------------\n",
      "training total loss: 5.2701\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9223  Type_loss: 3.3478\n",
      "validation total loss: 5.6170\n",
      "validation Maker_acc: 0.16  Type_acc: 0.49\n",
      "validation Maker_loss: 2.1286  Type_loss: 3.7629\n",
      "Epoch 55/59\n",
      "------------------------------\n",
      "training total loss: 5.2746\n",
      "training Maker_acc: 0.58  Type_acc: 0.69\n",
      "training Maker_loss: 1.9257  Type_loss: 3.3489\n",
      "validation total loss: 5.1121\n",
      "validation Maker_acc: 0.16  Type_acc: 0.50\n",
      "validation Maker_loss: 2.1232  Type_loss: 3.7652\n",
      "Epoch 56/59\n",
      "------------------------------\n",
      "training total loss: 5.2705\n",
      "training Maker_acc: 0.58  Type_acc: 0.70\n",
      "training Maker_loss: 1.9181  Type_loss: 3.3524\n",
      "validation total loss: 4.9334\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1322  Type_loss: 3.7648\n",
      "Epoch 57/59\n",
      "------------------------------\n",
      "training total loss: 5.2653\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9225  Type_loss: 3.3428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation total loss: 4.9428\n",
      "validation Maker_acc: 0.16  Type_acc: 0.50\n",
      "validation Maker_loss: 2.1215  Type_loss: 3.7634\n",
      "Epoch 58/59\n",
      "------------------------------\n",
      "training total loss: 5.2686\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9203  Type_loss: 3.3483\n",
      "validation total loss: 6.3669\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1324  Type_loss: 3.7638\n",
      "Epoch 59/59\n",
      "------------------------------\n",
      "training total loss: 5.2626\n",
      "training Maker_acc: 0.59  Type_acc: 0.70\n",
      "training Maker_loss: 1.9161  Type_loss: 3.3465\n",
      "validation total loss: 4.8820\n",
      "validation Maker_acc: 0.16  Type_acc: 0.48\n",
      "validation Maker_loss: 2.1397  Type_loss: 3.7647\n",
      "Training complete in 151m 41s\n",
      "Best val Acc: 4.581579\n"
     ]
    }
   ],
   "source": [
    "trained_model, history = train_model(MTL_model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryan_\\.conda\\envs\\azure_grabchallenge\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\ryan_\\.conda\\envs\\azure_grabchallenge\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training total loss: 5.7252\n",
      "training Maker_acc: 0.35  Type_acc: 0.50\n",
      "training Maker_loss: 2.1304  Type_loss: 3.5948\n",
      "validation total loss: 5.5937\n",
      "validation Maker_acc: 0.10  Type_acc: 0.30\n",
      "validation Maker_loss: 2.3095  Type_loss: 3.8306\n",
      "saving with loss of 5.593722820281982 improved over previous 150\n",
      "Epoch 1/2\n",
      "------------------------------\n",
      "training total loss: 5.5242\n",
      "training Maker_acc: 0.45  Type_acc: 0.58\n",
      "training Maker_loss: 2.0448  Type_loss: 3.4794\n",
      "validation total loss: 5.9109\n",
      "validation Maker_acc: 0.11  Type_acc: 0.40\n",
      "validation Maker_loss: 2.2237  Type_loss: 3.8202\n",
      "Epoch 2/2\n",
      "------------------------------\n",
      "training total loss: 5.4436\n",
      "training Maker_acc: 0.50  Type_acc: 0.60\n",
      "training Maker_loss: 2.0124  Type_loss: 3.4312\n",
      "validation total loss: 5.3985\n",
      "validation Maker_acc: 0.12  Type_acc: 0.40\n",
      "validation Maker_loss: 2.2147  Type_loss: 3.7996\n",
      "saving with loss of 5.39845085144043 improved over previous 5.593722820281982\n",
      "Training complete in 7m 38s\n",
      "Best val Acc: 5.398451\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model.state_dict(), 'sixth.pth')\n",
    "#'fifth_MTL.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "multi_task_model(\n",
       "  (base_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  )\n",
       "  (x1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=0.2, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (x2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (bn2): BatchNorm1d(512, eps=0.2, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (x3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (y_maker): Linear(in_features=256, out_features=49, bias=True)\n",
       "  (y_type): Linear(in_features=256, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fr_model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "#freeze layers\n",
    "for param in fr_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "print(fr_model)\n",
    "num_ftrs = fr_model.fc.in_features\n",
    "fr_model.fc = torch.nn.Linear(num_ftrs, 1024)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "MTL_model = multi_task_model(fr_model, class_maker, class_type)\n",
    "MTL_model = MTL_model.to(device)\n",
    "\n",
    "MTL_model.load_state_dict(torch.load('fifth_MTL.pth'))\n",
    "MTL_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_types = [\"Coupe\", \"Sedan\", \"Cab\",\n",
    "             \"Beetle\", \"SUV\", \"Van\",\n",
    "             \"SuperCab\", \"Convertible\",\n",
    "             \"Minivan\", \"Hatchback\",\n",
    "             \"Wagon\", \"no_type\"]\n",
    "\n",
    "makers = [\"AM General Hummer\", \"Acura\", \"Aston Martin\", \"Audi\" , \"BMW\", \"Bentley\", \"Bugatti Veyron 16.4\",\"Buick\",\"Cadillac\",\n",
    "         \"Chevrolet\", \"Chrysler\", \"Daewoo\", \"Dodge\", \"Eagle Talon\", \"FIAT\", \"Ferrari\", \"Ford\", \"GMC\", \"Geo Metro Convertible\", \n",
    "         \"HUMMER\", \"Honda\", \"Hyundai\", \"Infiniti\", \"Isuzu Ascender\", \"Jaguar\", \"Jeep\", \"Lamborghini\", \"Land Rover\", \n",
    "         \"Lincoln Town Car\", \"MINI Cooper Roadster\", \"Maybach Landaulet\", \"Mazda Tribute\", \"McLaren MP4-12C\", \"Mercedes-Benz\", \n",
    "         \"Mitsubishi Lancer\",\"Nissan\", \"Plymouth Neon\", \"Porsche Panamera\", \"Ram C/V\", \"Rolls-Royce\", \"Scion\", \"Spyker\", \"Suzuki\", \n",
    "         \"Tesla\", \"Toyota\", \"Volkswagen\", \"Volvo\", \"smart fortwo\", \"Fisker Karma Sedan\"]\n",
    "\n",
    "def extract_label(label_list, pred_array, top_n=1):\n",
    "    pred_max = torch.topk(pred_array,top_n)[1]\n",
    "    pasta = []\n",
    "    for tk in torch.topk(pred_array,5)[1][0]:\n",
    "\n",
    "        pasta.append(label_list[tk])\n",
    "    print(pasta)\n",
    "                     \n",
    "    out_list = []\n",
    "    for i in pred_max[0]:\n",
    "        out_list.append(label_list[i])\n",
    "    return out_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get accuracy (maker, model), #get ROC curve\n",
    "\n",
    "#fuzzy logic for some exact matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryan_\\.conda\\envs\\azure_grabchallenge\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\ryan_\\.conda\\envs\\azure_grabchallenge\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-072078cd5da4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMTL_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmakers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcar_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_label' is not defined"
     ]
    }
   ],
   "source": [
    "#evaluation mode\n",
    "from os import listdir\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = Image.open(image_name)\n",
    "    image = image_transformers[\"validation\"](image).float()\n",
    "    image = Variable(image, requires_grad=False)\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image.cuda()  #assumes that you're using GPU\n",
    "\n",
    "for image_file in dataset[\"test\"][:10]:\n",
    "    file_loc = os.path.join(root_dir,image_file[-1][0])\n",
    "    img = image_loader(file_loc)\n",
    "    \n",
    "    d = MTL_model(img)\n",
    "    print(extract_label(makers, d[0]), extract_label(car_types, d[1]))\n",
    "    print(meta_data[image_file[-2][0][0]-1])\n",
    "    \n",
    "    pil_im = Image.open(file_loc)\n",
    "    plt.imshow(np.asarray(pil_im))\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cfb170087af3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mValues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     targets.append([data[\"maker_ID\"].to(device),\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "Values = []\n",
    "targets = []\n",
    "for batch_idx, data in enumerate(testing):\n",
    "\n",
    "    targets.append([data[\"maker_ID\"].to(device),\n",
    "                    data[\"type_ID\"].to(device)])\n",
    "\n",
    "    if batch_idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "imsize = 224\n",
    "loader = transforms.Compose([transforms.Resize((imsize,imsize)),\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "#image test folder\n",
    "folder_path = \"\"\n",
    "\n",
    "\n",
    "#saber4.png\n",
    "\n",
    "image = image_loader('jalter3.jpg')\n",
    "y_pred = model1(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_test = utils.Load_Images(root_dir=\"testing_data/\", annotations_path=\"data/devkit/cars_test_annos.mat\",seed=10, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meta_data)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
