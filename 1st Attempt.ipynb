{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1ee4f7ec7dc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "\n",
    "import time\n",
    "import utils\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Load_Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = \"data/cars_train/\"\n",
    "car_annotations_path = \"data/devkit/cars_train_annos.mat\"\n",
    "car_metadata_path = \"data/devkit/cars_meta.mat\"\n",
    "\n",
    "#Load Meta Data\n",
    "meta_data = loadmat(car_metadata_path)\n",
    "meta_data = np.concatenate(meta_data[\"class_names\"][0])\n",
    "\n",
    "nb_classes = len(meta_data)\n",
    "\n",
    "dataset = Load_Images(root_dir = root_dir, annotations_path=car_annotations_path, seed=seed, train_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class car_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, files, root_dir, meta_data, image_transform=None):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "        #image file names\n",
    "        self.image_files = [file[-1][0] for file in files]\n",
    "        \n",
    "        #Class ID\n",
    "        #id needs to be adjusted by 1, for pytorch NLLosss \n",
    "        self.id = [file[-2][0] - 1 for file in files]\n",
    "        \n",
    "        #Class Name\n",
    "        self.class_name = [meta_data[file[-2][0] - 1][0] for file in files]\n",
    "        \n",
    "        #Get Car Year\n",
    "        self.carYear, self.carYear_ID\n",
    "\n",
    "        #Get Car Maker\n",
    "        \n",
    "        #Get Car Type\n",
    "    def __len__(self):\n",
    "        return len(self.id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        if self.image_transform:\n",
    "            img = self.image_transform(img)\n",
    "        \n",
    "        target = torch.from_numpy(np.array(self.id[idx]))[0]\n",
    "\n",
    "        sample = {'Image':img, 'class_ID':target, \"class_name\":self.class_name[idx]}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, class_type):\n",
    "                      \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        \n",
    "        self.num_samples = len(self.indices) \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx, class_type)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx, class_type)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx, class_type):\n",
    "        return dataset[idx][class_type].item()\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "                self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Training Weighted Random Sampler\n",
    "targets = [i[-2][0][0] for i in dataset[\"training\"]]\n",
    "class_sample_counts=[len(np.where(targets == t)[0]) for t in np.unique(targets)]\n",
    "weight = 1. / np.array(class_sample_counts)\n",
    "samples_weight = np.array([weight[t-1] for t in targets])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weight = samples_weight.double()\n",
    "training_sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=False)\n",
    "\n",
    "#batch size\n",
    "bs = 15\n",
    "\n",
    "image_transformers = {'train': transforms.Compose([transforms.Resize((244,244)),\n",
    "                                                   transforms.RandomRotation(degrees=50),\n",
    "                                                   transforms.RandomHorizontalFlip(0.8),\n",
    "                                                   transforms.ColorJitter(brightness=0.8, contrast=0.8),\n",
    "                                                   transforms.\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                        [0.229, 0.224, 0.225])\n",
    "                                                  ]),\n",
    "                      'validation': transforms.Compose([transforms.Resize((244,244)),\n",
    "                                                       transforms.ToTensor(),\n",
    "                                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                            [0.229, 0.224, 0.225])\n",
    "                                                       ])\n",
    "                     }\n",
    "\n",
    "#\n",
    "training_data = car_dataset(dataset[\"training\"],\n",
    "                            root_dir = root_dir,\n",
    "                            meta_data = meta_data,\n",
    "                            image_transform = image_transformers[\"train\"]\n",
    "                           )\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=bs, \n",
    "                                           sampler=ImbalancedDatasetSampler(training_data, \"class_ID\"))\n",
    "\n",
    "validation_data = car_dataset(dataset[\"validation\"], \n",
    "                             root_dir = root_dir,\n",
    "                             meta_data = meta_data,\n",
    "                             image_transform  = image_transformers[\"validation\"])\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=bs,\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ids = [i[\"class_name\"] for i in training_data]\n",
    "validation_ids = [i[\"class_name\"] for i in validation_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.Series(validation_ids).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: GeForce GTX 960\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=196, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Load the model based on VGG19\n",
    "vgg_based = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "'''\n",
    "for param in vgg_based.parameters():\n",
    "    param.requires_grad = False\n",
    "'''\n",
    "\n",
    "\n",
    "for idx,param in enumerate(vgg_based.parameters()):\n",
    "    if idx <= 35:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "# Modify the last layer\n",
    "number_features = vgg_based.classifier[6].in_features\n",
    "features = list(vgg_based.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([torch.nn.Linear(number_features, nb_classes)])\n",
    "vgg_based.classifier = torch.nn.Sequential(*features)\n",
    "\n",
    "vgg_based = vgg_based.to(device)\n",
    "\n",
    "print(vgg_based)\n",
    "#torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0)\n",
    "#\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = torch.optim.Adam(vgg_based.parameters(), lr= 0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_ft = torch.optim.Adam(vgg_based.parameters(), lr= 0.001, weight_decay=1e-5)\n",
    "#torch.optim.Adam(vgg_based.parameters(), lr= 0.0001, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=1):\n",
    "    since = time.time()\n",
    "    history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 30)\n",
    "\n",
    "        training_loss = 0\n",
    "        validation_loss = 0\n",
    "        \n",
    "        training_accuracy = 0\n",
    "        validation_accuracy = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            inputs = data[\"Image\"]\n",
    "            labels = data[\"class_ID\"]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs  = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            training_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            #train accuracy\n",
    "            (max_vals, arg_maxs) = torch.max(outputs, dim=1) \n",
    "            correct_counts = arg_maxs.eq(labels.data.view_as(arg_maxs))\n",
    "\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            training_accuracy += acc.item() * inputs.size(0)\n",
    "\n",
    "            \n",
    "        #get accuracy\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for batch_idx, data in enumerate(validation_loader):\n",
    "                inputs = data[\"Image\"]\n",
    "                labels = data[\"class_ID\"]\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.long()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                validation_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                #train accuracy\n",
    "                (max_vals, arg_maxs) = torch.max(outputs, dim=1) \n",
    "                correct_counts = arg_maxs.eq(labels.data.view_as(arg_maxs))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                validation_accuracy += acc.item() * inputs.size(0)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = training_loss/len(training_data)\n",
    "        avg_train_acc = training_accuracy/float(len(training_data))\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = validation_loss/len(validation_data)\n",
    "        avg_valid_acc = validation_accuracy/float(len(validation_data))\n",
    "        history.append([avg_train_loss, avg_train_acc, avg_valid_loss, avg_valid_acc])\n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%\".format(epoch + 1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "------------------------------\n",
      "Epoch : 001, Training: Loss: 5.5490, Accuracy: 6.6154%, \n",
      "\t\tValidation : Loss : 4.5182, Accuracy: 11.0086%\n",
      "Epoch 2/20\n",
      "------------------------------\n",
      "Epoch : 002, Training: Loss: 4.0712, Accuracy: 17.1846%, \n",
      "\t\tValidation : Loss : 4.1503, Accuracy: 12.4846%\n",
      "Epoch 3/20\n",
      "------------------------------\n",
      "Epoch : 003, Training: Loss: 3.6786, Accuracy: 21.9692%, \n",
      "\t\tValidation : Loss : 4.0453, Accuracy: 14.8216%\n",
      "Epoch 4/20\n",
      "------------------------------\n",
      "Epoch : 004, Training: Loss: 3.4784, Accuracy: 24.8923%, \n",
      "\t\tValidation : Loss : 4.0999, Accuracy: 15.2522%\n",
      "Epoch 5/20\n",
      "------------------------------\n",
      "Epoch : 005, Training: Loss: 3.2712, Accuracy: 28.3231%, \n",
      "\t\tValidation : Loss : 4.0482, Accuracy: 16.0517%\n",
      "Epoch 6/20\n",
      "------------------------------\n",
      "Epoch : 006, Training: Loss: 3.0963, Accuracy: 31.6923%, \n",
      "\t\tValidation : Loss : 4.1501, Accuracy: 15.1907%\n",
      "Epoch 7/20\n",
      "------------------------------\n",
      "Epoch : 007, Training: Loss: 2.9696, Accuracy: 33.6000%, \n",
      "\t\tValidation : Loss : 4.1200, Accuracy: 17.8967%\n",
      "Epoch 8/20\n",
      "------------------------------\n",
      "Epoch : 008, Training: Loss: 2.8905, Accuracy: 35.6308%, \n",
      "\t\tValidation : Loss : 4.2081, Accuracy: 18.3887%\n",
      "Epoch 9/20\n",
      "------------------------------\n",
      "Epoch : 009, Training: Loss: 2.7746, Accuracy: 37.0615%, \n",
      "\t\tValidation : Loss : 4.2085, Accuracy: 17.6507%\n",
      "Epoch 10/20\n",
      "------------------------------\n",
      "Epoch : 010, Training: Loss: 2.6166, Accuracy: 39.9231%, \n",
      "\t\tValidation : Loss : 4.1329, Accuracy: 19.7417%\n",
      "Epoch 11/20\n",
      "------------------------------\n",
      "Epoch : 011, Training: Loss: 2.5924, Accuracy: 40.5231%, \n",
      "\t\tValidation : Loss : 3.9958, Accuracy: 19.9262%\n",
      "Epoch 12/20\n",
      "------------------------------\n",
      "Epoch : 012, Training: Loss: 2.4685, Accuracy: 42.7077%, \n",
      "\t\tValidation : Loss : 4.1072, Accuracy: 20.1107%\n",
      "Epoch 13/20\n",
      "------------------------------\n",
      "Epoch : 013, Training: Loss: 2.3980, Accuracy: 43.4154%, \n",
      "\t\tValidation : Loss : 4.1297, Accuracy: 19.4957%\n",
      "Epoch 14/20\n",
      "------------------------------\n",
      "Epoch : 014, Training: Loss: 2.3703, Accuracy: 44.4462%, \n",
      "\t\tValidation : Loss : 4.0915, Accuracy: 20.2337%\n",
      "Epoch 15/20\n",
      "------------------------------\n",
      "Epoch : 015, Training: Loss: 2.2450, Accuracy: 46.4769%, \n",
      "\t\tValidation : Loss : 4.0192, Accuracy: 20.9102%\n",
      "Epoch 16/20\n",
      "------------------------------\n",
      "Epoch : 016, Training: Loss: 2.2107, Accuracy: 48.0000%, \n",
      "\t\tValidation : Loss : 4.1682, Accuracy: 21.2177%\n",
      "Epoch 17/20\n",
      "------------------------------\n",
      "Epoch : 017, Training: Loss: 2.1446, Accuracy: 49.0923%, \n",
      "\t\tValidation : Loss : 4.3948, Accuracy: 18.3272%\n",
      "Epoch 18/20\n",
      "------------------------------\n",
      "Epoch : 018, Training: Loss: 2.1176, Accuracy: 49.8769%, \n",
      "\t\tValidation : Loss : 4.3395, Accuracy: 20.9102%\n",
      "Epoch 19/20\n",
      "------------------------------\n",
      "Epoch : 019, Training: Loss: 2.0249, Accuracy: 51.2154%, \n",
      "\t\tValidation : Loss : 4.2111, Accuracy: 20.3567%\n",
      "Epoch 20/20\n",
      "------------------------------\n",
      "Epoch : 020, Training: Loss: 2.0339, Accuracy: 51.5231%, \n",
      "\t\tValidation : Loss : 4.3432, Accuracy: 19.6187%\n",
      "Training complete in 397m 41s\n"
     ]
    }
   ],
   "source": [
    "mods, history = train_model(vgg_based, criterion, optimizer_ft, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 99,  90, 166,  67,  53, 101,  84, 141, 178, 140, 133,  38,  18, 126,\n",
      "         23], dtype=torch.uint8)\n",
      "tensor([107,  49, 178,  34,  54,  67, 180,  66,  46,  37, 166,  81, 193,  39,\n",
      "        187], dtype=torch.uint8)\n",
      "tensor([ 14, 154, 150,  48, 127, 184,  10,  79, 172,  36, 193, 161,  85, 189,\n",
      "        126], dtype=torch.uint8)\n",
      "tensor([195,   7, 140,  50, 150,  14,   2, 193,  25, 149, 120,  81,  17,  42,\n",
      "        134], dtype=torch.uint8)\n",
      "tensor([145,  33,  73,  85,  61,   1, 195, 125, 189, 113, 163,   3,  59,  50,\n",
      "         44], dtype=torch.uint8)\n",
      "tensor([ 23, 112, 151, 184, 116, 141, 165, 155, 137,  73,   6, 125, 100,  57,\n",
      "         38], dtype=torch.uint8)\n",
      "tensor([116,  94, 145,  17, 163,  41, 188,  77,  36,  43,  19, 187, 173, 104,\n",
      "        158], dtype=torch.uint8)\n",
      "tensor([139, 101, 124, 156, 144,  71, 173,  29, 125,   2,  80,  96, 106, 192,\n",
      "         77], dtype=torch.uint8)\n",
      "tensor([ 66, 178, 192, 153, 131,  77,  15,  42, 150,  59,   7,  46, 171,  42,\n",
      "         36], dtype=torch.uint8)\n",
      "tensor([ 80, 116,  34, 137, 123,  52,   0,  37, 117,  59, 119, 150,  90, 178,\n",
      "        116], dtype=torch.uint8)\n",
      "tensor([ 83,  73,  43,  21,  64, 182, 133, 139, 170, 149, 125,  73,  83, 121,\n",
      "        133], dtype=torch.uint8)\n",
      "tensor([ 67,  96, 121,   9,  50,  22,  81,   0, 147, 174,  15, 194, 190, 145,\n",
      "         69], dtype=torch.uint8)\n",
      "tensor([112,   1, 185,  28, 156, 189,  75,  41, 174, 109, 108, 109, 190,  69,\n",
      "        138], dtype=torch.uint8)\n",
      "tensor([129, 108, 159,  64,  97, 115,  51,  82, 184, 123, 100, 166,  84,  65,\n",
      "         52], dtype=torch.uint8)\n",
      "tensor([ 27, 153,  46,  85, 158, 159, 119, 145, 150,  17, 175, 179,  32,  91,\n",
      "        178], dtype=torch.uint8)\n",
      "tensor([ 40,  23, 124,  95, 106,  99,  14, 192,  96, 167, 113,   3, 165, 104,\n",
      "         47], dtype=torch.uint8)\n",
      "tensor([47, 41, 12, 50, 28, 57, 16, 65, 98,  7, 78, 86, 40, 26, 21],\n",
      "       dtype=torch.uint8)\n",
      "tensor([ 76,  45,  35,  52, 195, 164,  39,  49,  74,  50,  25,  94,  89,  83,\n",
      "        187], dtype=torch.uint8)\n",
      "tensor([147, 191,  39,  44, 153, 155,  84,  38, 172,  14,  94,  37,  84, 101,\n",
      "        127], dtype=torch.uint8)\n",
      "tensor([ 95, 153,  32,  58, 155,  14,  34,  33,  87, 124, 136, 166,  98, 118,\n",
      "        163], dtype=torch.uint8)\n",
      "tensor([ 73,  53,  84, 107, 127,  43, 147, 182,  68,  10,  17,  57,  29,  39,\n",
      "          7], dtype=torch.uint8)\n",
      "tensor([ 12,  53,  86, 114, 122,  76, 164,  74, 141,  34,  42,   8,   2, 121,\n",
      "        119], dtype=torch.uint8)\n",
      "tensor([ 49,  81,   0,  34,  64, 145, 161,  80, 171,  78, 107,  64, 116, 107,\n",
      "        143], dtype=torch.uint8)\n",
      "tensor([118,  33, 177, 187,  45, 161, 172,   4, 194,   9,  14,  78, 122,  14,\n",
      "         13], dtype=torch.uint8)\n",
      "tensor([182, 125, 118,  99,  34,   0,  89,  18,  20,  36, 102, 101,  10,  74,\n",
      "        183], dtype=torch.uint8)\n",
      "tensor([ 26, 103,  93, 137,  17, 185, 179,  85,  35,  61,   7,  17,  20, 175,\n",
      "          7], dtype=torch.uint8)\n",
      "tensor([167,  90, 173, 110,  87,  69,  56, 114, 146,  40, 161,  52, 149, 134,\n",
      "        180], dtype=torch.uint8)\n",
      "tensor([136,   7, 124, 120, 144, 187,  36,  88,  43, 114, 169,  20, 144, 123,\n",
      "        122], dtype=torch.uint8)\n",
      "tensor([194, 193,  99, 194,   9, 106,  71,  82,   9, 139, 119, 121,  87,  81,\n",
      "         75], dtype=torch.uint8)\n",
      "tensor([  0, 117,  86, 168,  45, 182,  72,  54,  64, 183, 190,   4,  71,  58,\n",
      "        164], dtype=torch.uint8)\n",
      "tensor([ 23, 178, 162, 144, 108, 132, 124,  76,  86, 138,  58,  29,  61,  79,\n",
      "        152], dtype=torch.uint8)\n",
      "tensor([119, 138,  17,  45, 163,  51, 166,  30,  29, 101, 114,  45, 185,  89,\n",
      "        159], dtype=torch.uint8)\n",
      "tensor([189,  31, 178, 145, 121, 151,  46,  28, 155,  11, 128,   7, 137, 191,\n",
      "         29], dtype=torch.uint8)\n",
      "tensor([ 57,  48,  97, 180,  49, 160,  73, 125, 106, 193, 179,  59,  80,  54,\n",
      "          6], dtype=torch.uint8)\n",
      "tensor([ 15,  92, 165,  89, 162, 109, 179,  99,  84,  56,  24,  44,  93,  27,\n",
      "         36], dtype=torch.uint8)\n",
      "tensor([ 93, 113,  45, 145,  86,  25,  41, 187, 126,  49,  67, 103, 192, 184,\n",
      "         93], dtype=torch.uint8)\n",
      "tensor([175, 153,  36, 192,  71,  62,  46,  35, 146, 107, 128, 130, 169,  62,\n",
      "         97], dtype=torch.uint8)\n",
      "tensor([145,  12, 171, 108,  59, 121, 131, 113, 188, 166,  31,  94, 170, 115,\n",
      "         30], dtype=torch.uint8)\n",
      "tensor([137, 156, 174, 130,  68,  21,   3, 122, 182, 100,  99, 164, 169,  52,\n",
      "         26], dtype=torch.uint8)\n",
      "tensor([ 61, 169, 172,  28,  29,   7, 108,  47, 136, 164, 137,  93,  93, 159,\n",
      "         34], dtype=torch.uint8)\n",
      "tensor([  0,  92,  83,  18, 178, 113,  61, 152,  10,  54,   4, 150, 101,  76,\n",
      "         47], dtype=torch.uint8)\n",
      "tensor([  1, 164,  94,  23, 112,  11,  35,  93, 186, 119, 195,  95, 102,  22,\n",
      "        155], dtype=torch.uint8)\n",
      "tensor([134, 123,  27, 100, 166, 188, 153,  26, 185, 123,  25,  75,  38, 104,\n",
      "         47], dtype=torch.uint8)\n",
      "tensor([130, 105, 159,  41,  83,  49,   0, 160, 170,  82,  21,  75,  78, 139,\n",
      "         77], dtype=torch.uint8)\n",
      "tensor([162, 161,  69, 172,  73,  95, 135,  45,  97, 136,  15, 115,  42, 108,\n",
      "         67], dtype=torch.uint8)\n",
      "tensor([ 51, 180, 138,   6,  36,  55,   8, 177, 124,   5, 102, 130,  50, 127,\n",
      "        146], dtype=torch.uint8)\n",
      "tensor([186, 180, 116, 152,  66, 178, 187,  29, 183, 173,  83, 116,  57,   5,\n",
      "        152], dtype=torch.uint8)\n",
      "tensor([118,  55, 109,  61,  85, 174, 161,  74, 184,  79,  74, 109,  48,  50,\n",
      "        101], dtype=torch.uint8)\n",
      "tensor([ 39,  67, 111,  56,  64,  20, 176,  36, 138,  51,  79,  50, 120, 193,\n",
      "         61], dtype=torch.uint8)\n",
      "tensor([134,  88, 145, 131, 168, 111, 176,  54, 180,  88,  59,  54, 139,  12,\n",
      "        142], dtype=torch.uint8)\n",
      "tensor([135, 163,   7,  12,  34,   2,  88,  30,  72, 163,   5,  20, 120,  80,\n",
      "         24], dtype=torch.uint8)\n",
      "tensor([141, 176, 162,   3, 189,  55,  19,  57,  45,  28,  54,  49, 165, 165,\n",
      "          6], dtype=torch.uint8)\n",
      "tensor([ 30, 144,  86, 128, 143, 144,  80, 117,  95,  40,  28, 119,  74, 106,\n",
      "        107], dtype=torch.uint8)\n",
      "tensor([ 79,  11,  77, 122,  65,  92,  26,  22,  71, 159,  16, 144, 178,  38,\n",
      "        167], dtype=torch.uint8)\n",
      "tensor([ 36, 117,  17,  71,  77, 143, 114, 152,  14,  55, 165,   8, 117,  81,\n",
      "         92], dtype=torch.uint8)\n",
      "tensor([  3, 110, 159,  39, 147, 165,  47, 195, 115,  78,  85,  75,  34,  40,\n",
      "        170], dtype=torch.uint8)\n",
      "tensor([ 43,  35, 163,  12, 125,  87,  78, 125,  86,  59, 122, 130, 127, 174,\n",
      "          7], dtype=torch.uint8)\n",
      "tensor([ 60,   4,  96, 110, 144,  84, 143, 170, 152, 181,  19, 171, 171,  83,\n",
      "         42], dtype=torch.uint8)\n",
      "tensor([ 21, 115,  79, 128,   6,  10,  12, 106,  39,  60,   1, 134, 179, 194,\n",
      "        155], dtype=torch.uint8)\n",
      "tensor([ 19,  21, 149, 137,  66, 125,   2, 181, 115,  69,  57, 100, 103, 139,\n",
      "        128], dtype=torch.uint8)\n",
      "tensor([105,  89, 131, 144, 124,   3, 171, 169,  44,  16,  83, 176,  93, 107,\n",
      "        140], dtype=torch.uint8)\n",
      "tensor([168, 114,  95, 125, 194,  34,  84,  19,  61,   4,  81, 133, 119, 160,\n",
      "        114], dtype=torch.uint8)\n",
      "tensor([ 43,  62,   4,  18, 171, 149,  15, 102, 166,  42, 162,  13, 195,  91,\n",
      "         44], dtype=torch.uint8)\n",
      "tensor([ 48, 187,  18,  45, 133, 194,  22,   4,  58, 154,  53,   5, 190, 135,\n",
      "         80], dtype=torch.uint8)\n",
      "tensor([ 47,   3,   4,  83,  81, 136,   9,  25, 148, 172,  41,  28, 189, 108,\n",
      "        127], dtype=torch.uint8)\n",
      "tensor([192, 108,   4, 168, 128, 104,  21, 190, 124, 125,  19,  67, 124,  48,\n",
      "          8], dtype=torch.uint8)\n",
      "tensor([ 21,  83,  76,   8,   7,   0, 194,  77, 193,  98,  88, 131, 141,  53,\n",
      "         78], dtype=torch.uint8)\n",
      "tensor([148,  43, 119, 139, 102, 116, 112,  21,  52,  27,   3, 100,   2, 139,\n",
      "        163], dtype=torch.uint8)\n",
      "tensor([125,  35, 129,  36,  27, 140,  18, 120,  34,  88,  78,  69,  82,   8,\n",
      "        105], dtype=torch.uint8)\n",
      "tensor([166,  68, 129, 154,  16,  99,  29,  87, 123,   6, 168,  69,  10,  51,\n",
      "        142], dtype=torch.uint8)\n",
      "tensor([190, 156,  43,  33,  15, 158, 160, 192,  98, 119,  75,  71, 176, 182,\n",
      "         85], dtype=torch.uint8)\n",
      "tensor([142, 138,  14, 146, 190,  90,  50, 164, 168,  82, 118,  18, 195,  96,\n",
      "         19], dtype=torch.uint8)\n",
      "tensor([ 51, 136, 128,  69,  80,  79, 193, 100,  38,  91, 191,  50,  51,  83,\n",
      "         20], dtype=torch.uint8)\n",
      "tensor([ 86,  72,  34,  45, 117, 183,  23,  32, 141, 189, 150, 144,  86, 163,\n",
      "        104], dtype=torch.uint8)\n",
      "tensor([ 18, 164, 144,  19, 108,  28, 110, 178, 122,  74, 122, 133,  32, 181,\n",
      "          5], dtype=torch.uint8)\n",
      "tensor([102,  47, 129,  13, 134, 179,  32,  85, 152,  13, 165, 108, 124, 154,\n",
      "        169], dtype=torch.uint8)\n",
      "tensor([135, 111, 141, 108,  53, 176,  92,  58,  36, 152, 140,  83, 159,  13,\n",
      "         50], dtype=torch.uint8)\n",
      "tensor([191, 186,  54,  11,  81,  40, 143,  26, 136,  97,  60,  27, 166, 127,\n",
      "        143], dtype=torch.uint8)\n",
      "tensor([ 76,  90,  79, 105, 121, 123, 145,  59, 133,   4,  44,  47,  58, 172,\n",
      "         88], dtype=torch.uint8)\n",
      "tensor([114, 138,  94, 157, 127,  14,  44,  47,  26, 186, 184, 164, 139, 124,\n",
      "         59], dtype=torch.uint8)\n",
      "tensor([ 45,  50, 160,  36, 172,  89,  25, 144,  62,  89,  21,  20,  89, 104,\n",
      "        194], dtype=torch.uint8)\n",
      "tensor([174, 163, 177,  69,  62, 140, 185,  47, 114, 186, 100,  15, 140, 170,\n",
      "         89], dtype=torch.uint8)\n",
      "tensor([ 41,  70,  71,  15,   7,  26, 121,  67,  68, 101, 110,   7, 137,  66,\n",
      "        111], dtype=torch.uint8)\n",
      "tensor([ 98,  65, 128, 132, 173,  56, 168,   4,  28,  71,  10,  87,  22, 148,\n",
      "        152], dtype=torch.uint8)\n",
      "tensor([179, 157, 146, 148,   2,  72, 174, 103, 145,  87, 142,  84, 130, 118,\n",
      "        162], dtype=torch.uint8)\n",
      "tensor([185,  82,  13,  93,  81, 140, 105, 133, 159,  72,  23,  25, 186, 158,\n",
      "        160], dtype=torch.uint8)\n",
      "tensor([173, 121, 177, 158,  80, 105, 153, 118, 191, 116, 193, 193,  12, 134,\n",
      "        137], dtype=torch.uint8)\n",
      "tensor([ 31, 178, 152,  44,  20, 122,  10,  96,  58,  64, 159, 189, 185,  33,\n",
      "         65], dtype=torch.uint8)\n",
      "tensor([ 30, 190,  53, 108, 170, 119,  72,  75, 115, 118,  75,  75, 153,   3,\n",
      "         23], dtype=torch.uint8)\n",
      "tensor([ 42,  52,  33,  42, 110, 118,  52,  93,  33,  78, 131,  77, 130, 118,\n",
      "         23], dtype=torch.uint8)\n",
      "tensor([100,  95, 180,  70,   8, 186, 133,  65, 101, 181,   1,  77, 130,  38,\n",
      "         89], dtype=torch.uint8)\n",
      "tensor([ 94, 184, 117,  78,  22,   8, 143,  35, 115,  32, 125, 107,  55, 103,\n",
      "         55], dtype=torch.uint8)\n",
      "tensor([106, 162, 132,  23,  42,  26, 148, 194,  15,  47,  89,  90, 151, 158,\n",
      "        195], dtype=torch.uint8)\n",
      "tensor([ 18, 105, 159, 186, 173,  96, 154, 103,  80,   5,   9,  30,  28, 188,\n",
      "        124], dtype=torch.uint8)\n",
      "tensor([ 95,  39, 122,  65, 150, 183,  85, 111,  50, 142,  88, 164,  97,  10,\n",
      "        127], dtype=torch.uint8)\n",
      "tensor([113, 158,   8,  37,  67, 109,  88,   9, 170, 150,  86,  44, 118, 178,\n",
      "         71], dtype=torch.uint8)\n",
      "tensor([ 34, 135,  68, 174, 166,  82,  13,  58, 166,  39, 175,  38,  50,  88,\n",
      "        175], dtype=torch.uint8)\n",
      "tensor([132,  53, 118,  63,   4,  14,  51, 117,  77,  95, 189,  12,  22, 109,\n",
      "          2], dtype=torch.uint8)\n",
      "tensor([188,  99,  61, 182, 148, 144,  15, 166, 128, 133, 153,  92,  50,  71,\n",
      "        161], dtype=torch.uint8)\n",
      "tensor([ 71,  52, 166, 159, 154, 123,  48,  97,  61,  78,  72,  70, 147, 145,\n",
      "        122], dtype=torch.uint8)\n",
      "tensor([195, 188, 153,  63, 193, 115, 169, 118,  48,  27, 193,  27,  72, 176,\n",
      "        153], dtype=torch.uint8)\n",
      "tensor([100, 112, 181,  43,  43, 186, 171,  96,  27, 101, 177, 124, 114, 117,\n",
      "         94], dtype=torch.uint8)\n",
      "tensor([ 75,  49, 136,   3,  72, 161,  89, 158, 101,  10,  75, 185, 108,  63,\n",
      "        167], dtype=torch.uint8)\n",
      "tensor([ 59,  79,  26, 177, 173,  14, 150, 185, 162, 176,  23,  55, 122,  82,\n",
      "         97], dtype=torch.uint8)\n",
      "tensor([  2,  68, 117, 131, 176,  63,  62,  58,  84, 111,  95, 193, 115, 172,\n",
      "        146], dtype=torch.uint8)\n",
      "tensor([144, 111, 113, 141, 195, 160, 164, 174,  42,  13, 114,  51, 175, 176,\n",
      "        119], dtype=torch.uint8)\n",
      "tensor([ 55,  91, 189,  23, 168, 142, 118, 179, 125,  44, 179, 173, 137, 129,\n",
      "         55], dtype=torch.uint8)\n",
      "tensor([ 53,  64, 126, 109,  70, 151, 130,   1, 176, 114,   0, 182, 106,  89,\n",
      "        152], dtype=torch.uint8)\n",
      "tensor([175,  51, 178, 180,  11,  27], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    yhats = []  \n",
    "    ys = []\n",
    "    mods.eval()\n",
    "\n",
    "    for batch_idx, data in enumerate(validation_loader):\n",
    "        inputs = data[\"Image\"]\n",
    "        labels = data[\"class_ID\"]\n",
    "        print(labels) \n",
    "        ys.append(labels)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.long()\n",
    "        yhats.append(mods(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in yhats:\n",
    "    for g in i:\n",
    "        _, ind = torch.max(g, 0)\n",
    "        preds.append(ind.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1626"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-30.7764, -24.6068, -24.3167, -25.3581, -25.2567, -26.7324, -26.9435,\n",
      "        -26.5169, -24.0805, -27.1586, -31.0495, -24.8408, -19.7645, -19.7133,\n",
      "        -22.8920, -26.2908, -25.9389, -26.6877, -23.5790, -21.7163, -22.5169,\n",
      "        -20.1339, -22.8742, -22.3489, -26.0003, -22.5449, -29.9739, -24.8774,\n",
      "        -23.9268, -21.9241, -25.5971, -20.9475, -25.0463, -25.7017, -22.4867,\n",
      "        -27.2553, -22.0620, -28.2578, -23.7665, -21.4868, -21.2761, -23.5089,\n",
      "        -23.2199, -21.5725, -28.0328, -28.5681, -26.6380, -24.9598, -22.1308,\n",
      "        -20.8115, -23.9170, -21.1087, -21.2183, -26.0991, -28.5124, -25.6130,\n",
      "        -28.9545, -23.4986, -27.1342, -19.1432, -25.8685, -22.8357, -21.4335,\n",
      "        -27.4816, -24.2229, -28.4435, -22.4862, -23.3589, -27.0612, -25.0499,\n",
      "        -28.6977, -28.0180, -26.3431, -26.8075, -22.5624, -20.5459, -23.5431,\n",
      "        -26.0054, -21.3822, -25.7649, -25.8073, -27.0515, -22.9897, -23.8702,\n",
      "        -24.6176, -27.8255, -28.8613, -26.8510, -24.3094, -23.7919, -26.4364,\n",
      "        -22.2267, -25.8792, -23.0312, -19.7505, -25.3115, -25.2578, -23.3704,\n",
      "        -22.4133, -19.7766, -30.1969, -33.2825, -33.7131, -31.2566, -27.3831,\n",
      "        -24.2682, -26.9587, -28.5445, -26.9177, -22.8651, -23.8533, -28.9181,\n",
      "        -24.5132, -27.4208, -29.1995, -25.7710, -28.7348, -22.7565, -26.2041,\n",
      "        -21.8392, -20.8037, -25.3264, -28.2752, -25.7557, -27.7381, -24.9964,\n",
      "        -26.0246, -26.3365, -21.7440, -25.8917, -24.0302, -27.8009, -26.0648,\n",
      "        -24.9771, -24.4604, -29.1825, -22.5289, -28.6494, -23.3839, -22.6608,\n",
      "        -21.1371, -20.9479, -23.6934, -26.7857, -21.1274, -31.0230, -25.2564,\n",
      "        -20.3177, -22.6232, -33.5482, -28.3552, -27.4592, -33.2325, -22.8025,\n",
      "        -19.5347, -25.9487, -26.4110, -25.7552, -23.7145, -28.4811, -30.3747,\n",
      "        -22.7688, -24.2579, -21.3382, -22.2440, -31.2373, -27.4142, -27.8875,\n",
      "        -27.0004, -22.5535, -24.0427, -25.5383, -24.6001, -30.4141, -20.5547,\n",
      "        -22.3759, -20.4462, -23.4873, -28.1713, -25.1719, -26.6155, -23.5297,\n",
      "        -22.3780, -24.9511, -23.6952, -22.7634, -22.7893, -23.5404, -21.7594,\n",
      "        -25.3608, -24.0493, -25.4613, -24.0530, -28.3991, -19.2922, -26.2387],\n",
      "       device='cuda:0') tensor([ 3,  9,  9, 14,  9,  9,  9, 14,  7,  9,  7,  7, 14,  7,  7,  9,  7,  3,\n",
      "        14,  7,  7, 14,  7,  7, 14,  7, 14,  9, 14,  7,  7,  7,  7, 12,  7,  9,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7, 14,  9,  9,  9,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  9,  7,  9,  9,  3,  9,  3,  9,  7,  9,  7,  3,  3,  9,  9,\n",
      "         7,  3,  7,  7,  7,  9,  7,  7,  7,  7,  9,  9,  9,  9,  3,  3,  7,  7,\n",
      "         7,  7, 14,  7,  7, 14,  7,  7,  7,  0,  7,  7, 14, 14,  7,  7, 14,  7,\n",
      "         7,  7,  3,  7,  7,  7,  7,  3,  9,  7,  3,  7,  7,  3,  3,  7,  7,  7,\n",
      "         7,  9,  9,  7,  7,  9,  7,  9,  9,  9,  9,  9,  7,  9,  9,  7,  7,  9,\n",
      "         7,  7,  7,  7,  7, 12,  9,  7,  7,  7,  7,  9,  7,  7,  7,  9,  7,  7,\n",
      "        12,  7,  7,  9, 14,  7,  3,  7,  9,  9,  7,  3,  7,  7,  7,  7,  7,  7,\n",
      "         9,  7,  7,  7, 12,  7,  9,  9,  7,  9,  7,  7, 14,  7,  7,  7],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "values, indices = torch.max(yhats[0], 0)\n",
    "print(values, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 99,  90, 166,  67,  53, 101,  84, 141, 178, 140, 133,  38,  18, 126,\n",
       "          23], dtype=torch.uint8),\n",
       " tensor([107,  49, 178,  34,  54,  67, 180,  66,  46,  37, 166,  81, 193,  39,\n",
       "         187], dtype=torch.uint8),\n",
       " tensor([ 14, 154, 150,  48, 127, 184,  10,  79, 172,  36, 193, 161,  85, 189,\n",
       "         126], dtype=torch.uint8),\n",
       " tensor([195,   7, 140,  50, 150,  14,   2, 193,  25, 149, 120,  81,  17,  42,\n",
       "         134], dtype=torch.uint8),\n",
       " tensor([145,  33,  73,  85,  61,   1, 195, 125, 189, 113, 163,   3,  59,  50,\n",
       "          44], dtype=torch.uint8),\n",
       " tensor([ 23, 112, 151, 184, 116, 141, 165, 155, 137,  73,   6, 125, 100,  57,\n",
       "          38], dtype=torch.uint8),\n",
       " tensor([116,  94, 145,  17, 163,  41, 188,  77,  36,  43,  19, 187, 173, 104,\n",
       "         158], dtype=torch.uint8),\n",
       " tensor([139, 101, 124, 156, 144,  71, 173,  29, 125,   2,  80,  96, 106, 192,\n",
       "          77], dtype=torch.uint8),\n",
       " tensor([ 66, 178, 192, 153, 131,  77,  15,  42, 150,  59,   7,  46, 171,  42,\n",
       "          36], dtype=torch.uint8),\n",
       " tensor([ 80, 116,  34, 137, 123,  52,   0,  37, 117,  59, 119, 150,  90, 178,\n",
       "         116], dtype=torch.uint8),\n",
       " tensor([ 83,  73,  43,  21,  64, 182, 133, 139, 170, 149, 125,  73,  83, 121,\n",
       "         133], dtype=torch.uint8),\n",
       " tensor([ 67,  96, 121,   9,  50,  22,  81,   0, 147, 174,  15, 194, 190, 145,\n",
       "          69], dtype=torch.uint8),\n",
       " tensor([112,   1, 185,  28, 156, 189,  75,  41, 174, 109, 108, 109, 190,  69,\n",
       "         138], dtype=torch.uint8),\n",
       " tensor([129, 108, 159,  64,  97, 115,  51,  82, 184, 123, 100, 166,  84,  65,\n",
       "          52], dtype=torch.uint8),\n",
       " tensor([ 27, 153,  46,  85, 158, 159, 119, 145, 150,  17, 175, 179,  32,  91,\n",
       "         178], dtype=torch.uint8),\n",
       " tensor([ 40,  23, 124,  95, 106,  99,  14, 192,  96, 167, 113,   3, 165, 104,\n",
       "          47], dtype=torch.uint8),\n",
       " tensor([47, 41, 12, 50, 28, 57, 16, 65, 98,  7, 78, 86, 40, 26, 21],\n",
       "        dtype=torch.uint8),\n",
       " tensor([ 76,  45,  35,  52, 195, 164,  39,  49,  74,  50,  25,  94,  89,  83,\n",
       "         187], dtype=torch.uint8),\n",
       " tensor([147, 191,  39,  44, 153, 155,  84,  38, 172,  14,  94,  37,  84, 101,\n",
       "         127], dtype=torch.uint8),\n",
       " tensor([ 95, 153,  32,  58, 155,  14,  34,  33,  87, 124, 136, 166,  98, 118,\n",
       "         163], dtype=torch.uint8),\n",
       " tensor([ 73,  53,  84, 107, 127,  43, 147, 182,  68,  10,  17,  57,  29,  39,\n",
       "           7], dtype=torch.uint8),\n",
       " tensor([ 12,  53,  86, 114, 122,  76, 164,  74, 141,  34,  42,   8,   2, 121,\n",
       "         119], dtype=torch.uint8),\n",
       " tensor([ 49,  81,   0,  34,  64, 145, 161,  80, 171,  78, 107,  64, 116, 107,\n",
       "         143], dtype=torch.uint8),\n",
       " tensor([118,  33, 177, 187,  45, 161, 172,   4, 194,   9,  14,  78, 122,  14,\n",
       "          13], dtype=torch.uint8),\n",
       " tensor([182, 125, 118,  99,  34,   0,  89,  18,  20,  36, 102, 101,  10,  74,\n",
       "         183], dtype=torch.uint8),\n",
       " tensor([ 26, 103,  93, 137,  17, 185, 179,  85,  35,  61,   7,  17,  20, 175,\n",
       "           7], dtype=torch.uint8),\n",
       " tensor([167,  90, 173, 110,  87,  69,  56, 114, 146,  40, 161,  52, 149, 134,\n",
       "         180], dtype=torch.uint8),\n",
       " tensor([136,   7, 124, 120, 144, 187,  36,  88,  43, 114, 169,  20, 144, 123,\n",
       "         122], dtype=torch.uint8),\n",
       " tensor([194, 193,  99, 194,   9, 106,  71,  82,   9, 139, 119, 121,  87,  81,\n",
       "          75], dtype=torch.uint8),\n",
       " tensor([  0, 117,  86, 168,  45, 182,  72,  54,  64, 183, 190,   4,  71,  58,\n",
       "         164], dtype=torch.uint8),\n",
       " tensor([ 23, 178, 162, 144, 108, 132, 124,  76,  86, 138,  58,  29,  61,  79,\n",
       "         152], dtype=torch.uint8),\n",
       " tensor([119, 138,  17,  45, 163,  51, 166,  30,  29, 101, 114,  45, 185,  89,\n",
       "         159], dtype=torch.uint8),\n",
       " tensor([189,  31, 178, 145, 121, 151,  46,  28, 155,  11, 128,   7, 137, 191,\n",
       "          29], dtype=torch.uint8),\n",
       " tensor([ 57,  48,  97, 180,  49, 160,  73, 125, 106, 193, 179,  59,  80,  54,\n",
       "           6], dtype=torch.uint8),\n",
       " tensor([ 15,  92, 165,  89, 162, 109, 179,  99,  84,  56,  24,  44,  93,  27,\n",
       "          36], dtype=torch.uint8),\n",
       " tensor([ 93, 113,  45, 145,  86,  25,  41, 187, 126,  49,  67, 103, 192, 184,\n",
       "          93], dtype=torch.uint8),\n",
       " tensor([175, 153,  36, 192,  71,  62,  46,  35, 146, 107, 128, 130, 169,  62,\n",
       "          97], dtype=torch.uint8),\n",
       " tensor([145,  12, 171, 108,  59, 121, 131, 113, 188, 166,  31,  94, 170, 115,\n",
       "          30], dtype=torch.uint8),\n",
       " tensor([137, 156, 174, 130,  68,  21,   3, 122, 182, 100,  99, 164, 169,  52,\n",
       "          26], dtype=torch.uint8),\n",
       " tensor([ 61, 169, 172,  28,  29,   7, 108,  47, 136, 164, 137,  93,  93, 159,\n",
       "          34], dtype=torch.uint8),\n",
       " tensor([  0,  92,  83,  18, 178, 113,  61, 152,  10,  54,   4, 150, 101,  76,\n",
       "          47], dtype=torch.uint8),\n",
       " tensor([  1, 164,  94,  23, 112,  11,  35,  93, 186, 119, 195,  95, 102,  22,\n",
       "         155], dtype=torch.uint8),\n",
       " tensor([134, 123,  27, 100, 166, 188, 153,  26, 185, 123,  25,  75,  38, 104,\n",
       "          47], dtype=torch.uint8),\n",
       " tensor([130, 105, 159,  41,  83,  49,   0, 160, 170,  82,  21,  75,  78, 139,\n",
       "          77], dtype=torch.uint8),\n",
       " tensor([162, 161,  69, 172,  73,  95, 135,  45,  97, 136,  15, 115,  42, 108,\n",
       "          67], dtype=torch.uint8),\n",
       " tensor([ 51, 180, 138,   6,  36,  55,   8, 177, 124,   5, 102, 130,  50, 127,\n",
       "         146], dtype=torch.uint8),\n",
       " tensor([186, 180, 116, 152,  66, 178, 187,  29, 183, 173,  83, 116,  57,   5,\n",
       "         152], dtype=torch.uint8),\n",
       " tensor([118,  55, 109,  61,  85, 174, 161,  74, 184,  79,  74, 109,  48,  50,\n",
       "         101], dtype=torch.uint8),\n",
       " tensor([ 39,  67, 111,  56,  64,  20, 176,  36, 138,  51,  79,  50, 120, 193,\n",
       "          61], dtype=torch.uint8),\n",
       " tensor([134,  88, 145, 131, 168, 111, 176,  54, 180,  88,  59,  54, 139,  12,\n",
       "         142], dtype=torch.uint8),\n",
       " tensor([135, 163,   7,  12,  34,   2,  88,  30,  72, 163,   5,  20, 120,  80,\n",
       "          24], dtype=torch.uint8),\n",
       " tensor([141, 176, 162,   3, 189,  55,  19,  57,  45,  28,  54,  49, 165, 165,\n",
       "           6], dtype=torch.uint8),\n",
       " tensor([ 30, 144,  86, 128, 143, 144,  80, 117,  95,  40,  28, 119,  74, 106,\n",
       "         107], dtype=torch.uint8),\n",
       " tensor([ 79,  11,  77, 122,  65,  92,  26,  22,  71, 159,  16, 144, 178,  38,\n",
       "         167], dtype=torch.uint8),\n",
       " tensor([ 36, 117,  17,  71,  77, 143, 114, 152,  14,  55, 165,   8, 117,  81,\n",
       "          92], dtype=torch.uint8),\n",
       " tensor([  3, 110, 159,  39, 147, 165,  47, 195, 115,  78,  85,  75,  34,  40,\n",
       "         170], dtype=torch.uint8),\n",
       " tensor([ 43,  35, 163,  12, 125,  87,  78, 125,  86,  59, 122, 130, 127, 174,\n",
       "           7], dtype=torch.uint8),\n",
       " tensor([ 60,   4,  96, 110, 144,  84, 143, 170, 152, 181,  19, 171, 171,  83,\n",
       "          42], dtype=torch.uint8),\n",
       " tensor([ 21, 115,  79, 128,   6,  10,  12, 106,  39,  60,   1, 134, 179, 194,\n",
       "         155], dtype=torch.uint8),\n",
       " tensor([ 19,  21, 149, 137,  66, 125,   2, 181, 115,  69,  57, 100, 103, 139,\n",
       "         128], dtype=torch.uint8),\n",
       " tensor([105,  89, 131, 144, 124,   3, 171, 169,  44,  16,  83, 176,  93, 107,\n",
       "         140], dtype=torch.uint8),\n",
       " tensor([168, 114,  95, 125, 194,  34,  84,  19,  61,   4,  81, 133, 119, 160,\n",
       "         114], dtype=torch.uint8),\n",
       " tensor([ 43,  62,   4,  18, 171, 149,  15, 102, 166,  42, 162,  13, 195,  91,\n",
       "          44], dtype=torch.uint8),\n",
       " tensor([ 48, 187,  18,  45, 133, 194,  22,   4,  58, 154,  53,   5, 190, 135,\n",
       "          80], dtype=torch.uint8),\n",
       " tensor([ 47,   3,   4,  83,  81, 136,   9,  25, 148, 172,  41,  28, 189, 108,\n",
       "         127], dtype=torch.uint8),\n",
       " tensor([192, 108,   4, 168, 128, 104,  21, 190, 124, 125,  19,  67, 124,  48,\n",
       "           8], dtype=torch.uint8),\n",
       " tensor([ 21,  83,  76,   8,   7,   0, 194,  77, 193,  98,  88, 131, 141,  53,\n",
       "          78], dtype=torch.uint8),\n",
       " tensor([148,  43, 119, 139, 102, 116, 112,  21,  52,  27,   3, 100,   2, 139,\n",
       "         163], dtype=torch.uint8),\n",
       " tensor([125,  35, 129,  36,  27, 140,  18, 120,  34,  88,  78,  69,  82,   8,\n",
       "         105], dtype=torch.uint8),\n",
       " tensor([166,  68, 129, 154,  16,  99,  29,  87, 123,   6, 168,  69,  10,  51,\n",
       "         142], dtype=torch.uint8),\n",
       " tensor([190, 156,  43,  33,  15, 158, 160, 192,  98, 119,  75,  71, 176, 182,\n",
       "          85], dtype=torch.uint8),\n",
       " tensor([142, 138,  14, 146, 190,  90,  50, 164, 168,  82, 118,  18, 195,  96,\n",
       "          19], dtype=torch.uint8),\n",
       " tensor([ 51, 136, 128,  69,  80,  79, 193, 100,  38,  91, 191,  50,  51,  83,\n",
       "          20], dtype=torch.uint8),\n",
       " tensor([ 86,  72,  34,  45, 117, 183,  23,  32, 141, 189, 150, 144,  86, 163,\n",
       "         104], dtype=torch.uint8),\n",
       " tensor([ 18, 164, 144,  19, 108,  28, 110, 178, 122,  74, 122, 133,  32, 181,\n",
       "           5], dtype=torch.uint8),\n",
       " tensor([102,  47, 129,  13, 134, 179,  32,  85, 152,  13, 165, 108, 124, 154,\n",
       "         169], dtype=torch.uint8),\n",
       " tensor([135, 111, 141, 108,  53, 176,  92,  58,  36, 152, 140,  83, 159,  13,\n",
       "          50], dtype=torch.uint8),\n",
       " tensor([191, 186,  54,  11,  81,  40, 143,  26, 136,  97,  60,  27, 166, 127,\n",
       "         143], dtype=torch.uint8),\n",
       " tensor([ 76,  90,  79, 105, 121, 123, 145,  59, 133,   4,  44,  47,  58, 172,\n",
       "          88], dtype=torch.uint8),\n",
       " tensor([114, 138,  94, 157, 127,  14,  44,  47,  26, 186, 184, 164, 139, 124,\n",
       "          59], dtype=torch.uint8),\n",
       " tensor([ 45,  50, 160,  36, 172,  89,  25, 144,  62,  89,  21,  20,  89, 104,\n",
       "         194], dtype=torch.uint8),\n",
       " tensor([174, 163, 177,  69,  62, 140, 185,  47, 114, 186, 100,  15, 140, 170,\n",
       "          89], dtype=torch.uint8),\n",
       " tensor([ 41,  70,  71,  15,   7,  26, 121,  67,  68, 101, 110,   7, 137,  66,\n",
       "         111], dtype=torch.uint8),\n",
       " tensor([ 98,  65, 128, 132, 173,  56, 168,   4,  28,  71,  10,  87,  22, 148,\n",
       "         152], dtype=torch.uint8),\n",
       " tensor([179, 157, 146, 148,   2,  72, 174, 103, 145,  87, 142,  84, 130, 118,\n",
       "         162], dtype=torch.uint8),\n",
       " tensor([185,  82,  13,  93,  81, 140, 105, 133, 159,  72,  23,  25, 186, 158,\n",
       "         160], dtype=torch.uint8),\n",
       " tensor([173, 121, 177, 158,  80, 105, 153, 118, 191, 116, 193, 193,  12, 134,\n",
       "         137], dtype=torch.uint8),\n",
       " tensor([ 31, 178, 152,  44,  20, 122,  10,  96,  58,  64, 159, 189, 185,  33,\n",
       "          65], dtype=torch.uint8),\n",
       " tensor([ 30, 190,  53, 108, 170, 119,  72,  75, 115, 118,  75,  75, 153,   3,\n",
       "          23], dtype=torch.uint8),\n",
       " tensor([ 42,  52,  33,  42, 110, 118,  52,  93,  33,  78, 131,  77, 130, 118,\n",
       "          23], dtype=torch.uint8),\n",
       " tensor([100,  95, 180,  70,   8, 186, 133,  65, 101, 181,   1,  77, 130,  38,\n",
       "          89], dtype=torch.uint8),\n",
       " tensor([ 94, 184, 117,  78,  22,   8, 143,  35, 115,  32, 125, 107,  55, 103,\n",
       "          55], dtype=torch.uint8),\n",
       " tensor([106, 162, 132,  23,  42,  26, 148, 194,  15,  47,  89,  90, 151, 158,\n",
       "         195], dtype=torch.uint8),\n",
       " tensor([ 18, 105, 159, 186, 173,  96, 154, 103,  80,   5,   9,  30,  28, 188,\n",
       "         124], dtype=torch.uint8),\n",
       " tensor([ 95,  39, 122,  65, 150, 183,  85, 111,  50, 142,  88, 164,  97,  10,\n",
       "         127], dtype=torch.uint8),\n",
       " tensor([113, 158,   8,  37,  67, 109,  88,   9, 170, 150,  86,  44, 118, 178,\n",
       "          71], dtype=torch.uint8),\n",
       " tensor([ 34, 135,  68, 174, 166,  82,  13,  58, 166,  39, 175,  38,  50,  88,\n",
       "         175], dtype=torch.uint8),\n",
       " tensor([132,  53, 118,  63,   4,  14,  51, 117,  77,  95, 189,  12,  22, 109,\n",
       "           2], dtype=torch.uint8),\n",
       " tensor([188,  99,  61, 182, 148, 144,  15, 166, 128, 133, 153,  92,  50,  71,\n",
       "         161], dtype=torch.uint8),\n",
       " tensor([ 71,  52, 166, 159, 154, 123,  48,  97,  61,  78,  72,  70, 147, 145,\n",
       "         122], dtype=torch.uint8),\n",
       " tensor([195, 188, 153,  63, 193, 115, 169, 118,  48,  27, 193,  27,  72, 176,\n",
       "         153], dtype=torch.uint8),\n",
       " tensor([100, 112, 181,  43,  43, 186, 171,  96,  27, 101, 177, 124, 114, 117,\n",
       "          94], dtype=torch.uint8),\n",
       " tensor([ 75,  49, 136,   3,  72, 161,  89, 158, 101,  10,  75, 185, 108,  63,\n",
       "         167], dtype=torch.uint8),\n",
       " tensor([ 59,  79,  26, 177, 173,  14, 150, 185, 162, 176,  23,  55, 122,  82,\n",
       "          97], dtype=torch.uint8),\n",
       " tensor([  2,  68, 117, 131, 176,  63,  62,  58,  84, 111,  95, 193, 115, 172,\n",
       "         146], dtype=torch.uint8),\n",
       " tensor([144, 111, 113, 141, 195, 160, 164, 174,  42,  13, 114,  51, 175, 176,\n",
       "         119], dtype=torch.uint8),\n",
       " tensor([ 55,  91, 189,  23, 168, 142, 118, 179, 125,  44, 179, 173, 137, 129,\n",
       "          55], dtype=torch.uint8),\n",
       " tensor([ 53,  64, 126, 109,  70, 151, 130,   1, 176, 114,   0, 182, 106,  89,\n",
       "         152], dtype=torch.uint8),\n",
       " tensor([175,  51, 178, 180,  11,  27], dtype=torch.uint8)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for i in ys:\n",
    "    for g in i.numpy():\n",
    "        targets.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1626"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27929e9e6d8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XlclOX+//HXxc6wCiii4I4L7opplmWb2ylLM7XTZot9+5Ytp9M5eU6bbd/K03Lafqc81bHMUjMzNTuWle2amLjhAioqiIiA7DjMcP3+uAdEBBmF4R6Gz/PxmAczc19zz4eb8e09133d16201gghhPAsXmYXIIQQoulJuAshhAeScBdCCA8k4S6EEB5Iwl0IITyQhLsQQnggCXchhPBAEu5CCOGBJNyFEMID+Zj1xlFRUbpLly5mvb0QQrRImzZtOqa1bttQO9PCvUuXLiQlJZn19kII0SIppQ440066ZYQQwgNJuAshhAeScBdCCA9kWp97XSoqKsjIyKC8vNzsUkQ9AgICiI2NxdfX1+xShBBn4FbhnpGRQUhICF26dEEpZXY5ohatNbm5uWRkZNC1a1ezyxFCnIFbdcuUl5cTGRkpwe6mlFJERkbKNyshWgC3CndAgt3Nyd9HiJbBrbplhBBCADYrlOVBaa7jlnfyp5Mk3IUQwtXsFXAstUZYO4K6vgC3FjX6LSXc3diMGTO48sormTJlCnfccQcPPvggCQkJp7SZP38+SUlJvPHGGyZVKYSok70C9n8PO5bDrlVQln96G78QsEQ4bpEQFQ+BjvuW2j8jjWVP+jv19hLuLcQ777xjdglCiIbYK2D/D5CyHHauNALdLwR6jYf4KyCk/cmQtkSAj3NBfS6cCnel1DjgVcAbeEdr/Xyt5TOAfwCZjqfe0Fo3Ko2eXLmDlMOFjVnFaRI6hPLEVX0bbHfNNddw6NAhysvLuf/++7Hb7ezfv5+5c+cCxt7ypk2beP3113n66adZuHAhcXFxREVFMXToUB566KHT1rlz505uueUWfvvtNwDS09OZOHEiW7du5amnnmLlypWUlZUxcuRI3n777dMOXI4ePZoXX3yRxMRE/vOf//Dcc88RExNDz5498fev/wOycuVKnnnmGaxWK5GRkSxcuJDo6GiKi4u59957SUpKQinFE088wbXXXst///tf/v73v2O324mKiuKbb745m00sROtjt0H6D7DjM9i5yuhq8QuGXhOg7zXQ/TLwDWj2shoMd6WUN/AmcAWQAWxUSq3QWqfUarpYaz3LBTU2u/fee4+IiAjKysoYNmwY33zzDRdccEF1uC9evJhHHnmEpKQkPv30UzZv3ozNZmPIkCEMHTq0znX26dMHq9XKvn376NatG4sXL2bq1KkAzJo1i8cffxyAm266iVWrVnHVVVfVuZ6srCyeeOIJNm3aRFhYGJdccgmDBw+u93e58MILWb9+PUop3nnnHebOnctLL73E008/TVhYGNu2bQMgPz+fnJwcZs6cyQ8//EDXrl3Jy3P+4I0QrYrdBuk/OgJ9ZY1AHw8J10CPy8A30NQSndlzPw9I01rvA1BKLQKuBmqHe5NyZg/bVV577TU+++wzAA4dOsT+/fvp1q0b69evJz4+nt27d3PBBRfw6quvcvXVVxMYaPwR6wvkKlOnTmXJkiXMnj2bxYsXs3jxYgC+++475s6dS2lpKXl5efTt27fedW3YsIHRo0fTtq0x4+e0adPYs2dPve+ZkZHBtGnTyMrKwmq1Vp98tHbtWhYtWlTdrk2bNqxcuZKLLrqouk1ERIQzm0uI1qEq0Ku6XEpzjUDvOQ76TnKLQK/JmXDvCByq8TgDGF5Hu2uVUhcBe4A/aa0P1dHG7a1bt461a9fy66+/YrFYGD16NOXl5UybNo0lS5bQu3dvJk2ahFIKrfVZrXvatGlcd911TJ48GaUU8fHxlJeXc/fdd5OUlERcXBxz5sxp8CShsxlrfu+99/Lggw8yceJE1q1bx5w5cwDjbNPa66nrOSFatbLjkLnJCPOdK4xA9w0y9tD7XgM9LnerQK/JmZOY6vrXXjvVVgJdtNYDgLXA+3WuSKk7lVJJSqmknJycs6u0mRQUFNCmTRssFgu7du1i/fr1AEyePJnly5fz8ccfM23aNMDo8li5ciXl5eUUFxfzxRdfnHHd3bt3x9vbm6effrp6HVVBHhUVRXFxMUuXLj3jOoYPH866devIzc2loqKCTz75pMHfp2PHjgC8//7JP8uYMWNOGWGTn5/P+eefz/fff8/+/fsBpFtGtC4luZD2Dfz4Miy5BV4dBC90hg8nw9Yl0G00TF0Af90LU96FPle5bbCDc3vuGUBcjcexwOGaDbTWuTUe/ht4oa4Vaa3nAfMAEhMTz263t5mMGzeOt956iwEDBtCrVy9GjBgBGN0WCQkJpKSkcN555wEwbNgwJk6cyMCBA+ncuTOJiYmEhYWdcf3Tpk3jL3/5S3WAhoeHM3PmTPr370+XLl0YNmzYGV8fExPDnDlzOP/884mJiWHIkCHY7fZ628+ZM4frrruOjh07MmLEiOr3ffTRR7nnnnvo168f3t7ePPHEE0yePJl58+YxefJkKisradeuHV9//bXT206IFqPoCGRtOfVWUKOzIbwzxAyEwTdCzCDoPBL8LObVew5UQ10LSikfjK6WyzBGw2wE/qi13lGjTYzWOstxfxLwsNZ6xJnWm5iYqGtfiWnnzp306dPnXH4P0xQXFxMcHExpaSkXXXQR8+bNY8iQIWaX5VIt8e8kWimtoSCjRognGz+Ls0+2iexhBHnMQCPI2/c3him6KaXUJq11YkPtGtxz11rblFKzgDUYQyHf01rvUEo9BSRprVcA9ymlJgI2IA+Y0ajqW5A777yTlJQUysvLueWWWzw+2IVwa1pDbhrs/da4HfrNGMkCoLwgqhd0u8QI8g6DILofBISaW7OLODXOXWu9Glhd67nHa9z/G/C3pi2tZfjoo49Oe+6ee+7h559/PuW5+++/n1tvvdVldTz77LOn9b9fd911PPLIIy57TyHcQmmecSbo3m9h73cnu1fadDXGmncYZOyRR/dtcV0rjdFgt4yreEq3TGskfydhKpsVMjae3Ds/vBnQ4B8KXS+C7pdC90sgopvZlbpEk3XLCCGEqaq7Wr4zwjz9R7AWg/KG2EQYPdsI9A5DwFsirYpsCSGE+zlTV8uAacaeeZdREBhubp1uTMJdCOEebCdg1xeQvNAIdV15sqvlwj95dFeLK0i4CyHMlbUFNn8I2z4xZlEMjTXCvOc46WppBNlqzWj+/PmMGTOGDh06nNXr3nrrLSwWCzfffLOLKhOimZXkwrYlsHkhZG8Db3/oc6Vx0lDXi8HL2+wKWzwJ92Y0f/58+vXrV2e42+12vL3r/kDfddddri5NCNez22DvN8Ze+u4vobICOgyGCS9C/ykQ2MbsCj2K+4b7l7PhyLamXWf7/jD++QabuWI+96VLl5KUlMQNN9xAYGAgv/76K3369OG2227jq6++YtasWRQVFTFv3jysVis9evRgwYIFWCwW5syZQ3BwMA899BCjR49m+PDhfPfddxw/fpx3332XUaNG1fl7pKenc9NNN1FSUgLAG2+8wciRIwGYO3cuCxYswMvLi/Hjx/P888+TlpbGXXfdRU5ODt7e3nzyySd07979XLe2EIZjqUagb1kExUfAEgXn3QmDbzDGnguXcN9wN5Er5nOfMmUKb7zxRvUFN6oEBATw008/AZCbm8vMmTMBY+6Xd999l3vvvfe0ddlsNn777TdWr17Nk08+ydq1a+t8z6q5YQICAkhNTeX6668nKSmJL7/8kuXLl7NhwwYsFkv1BGE33HADs2fPZtKkSZSXl1NZWXnuG1G0buWFxlznyQvh0AZj2GL8GKPbJX4M+PiZXaHHc99wd2IP21VcNZ97XapmhwTYvn07jz76KMePH6e4uJixY8fW+ZrJkycDMHToUNLT0+tdd0VFBbNmzSI5ORlvb+/qed/Xrl3LrbfeisVinK0XERFBUVERmZmZTJo0CTD+0xHirB34FX5/H1I+h4pS43T/K56CAdMhJNrs6loV9w13k7hyPve6BAUFVd+fMWMGy5cvZ+DAgcyfP59169bV+Zqqy+p5e3tjs9nqXfcrr7xCdHQ0W7ZsobKysjqw65vLXYhzduAX+O7/jBOM/ENhwFQYfBN0HApyjQBTODOfe6viyvncQ0JCKCoqqnd5UVERMTExVFRUsHDhwib5XWJiYvDy8mLBggXVUwOPGTOG9957j9LSUsCYtz00NJTY2FiWL18OwIkTJ6qXC1Gvgxvgg6vhP+Ph2B4Y9wL8eTdc9apx9qgEu2kk3GsZN24cNpuNAQMG8Nhjj502n/uBAwfqnM998uTJDc7nPmPGDO666y4GDRpEWVnZacuffvpphg8fzhVXXEHv3r0b/bvcfffdvP/++4wYMYI9e/ZUf0sYN24cEydOJDExkUGDBvHiiy8CsGDBAl577TUGDBjAyJEjOXLkSKNrEB4qIwkWTIb3xkD2DhjzLNyXDCPualWTc7kzmTiskWQ+d9GqHN4M3z0HqWsgMAIufACG3QF+QQ2/VjQJmTismch87qJVyNoK656H3V9AQDhc9rgxnNE/xOzKRD0k3BvJHeZzX7NmDQ8//PApz3Xt2rV6xI8Q5yw7BdY9Z1wc2j8MLnkEht/lsRe48CRuF+51jeRoad58881mfb+xY8fWO2yyqcmomlYiZ7exp77jM/ALhosfhhF3yyyMLYhbhXtAQAC5ublERka2+ID3RFprcnNzZQy8JzuWCt+/ANuWGv3oo/4M59/j1tcUFXVzq3CPjY0lIyODnJwcs0sR9QgICCA2NtbsMkRTstuMA6VJ78LWxeATABfcDyPvg6BIs6sT58itwt3X15euXbuaXYYQnq2yErK3w/4fjNuBX8BaZIT6iLvhggcguK3ZVYpGcqtwF0K4gNZGd8v+740wT/8Jyoz5hIjsAQOuM65q1G20dL94EAl3ITxR/oGTe+b7fzBmYwTjQhi9xhtXN+oyCsI6mluncBkJdyE8QdER2P/jyb3z4weM54PaQddRRph3vci4BqkMVmgVJNyFaEm0Ni4WnZ0CR3cYp/5nbYXcVGN5QJixR37+LCPM2/aSMG+lJNyFcFdlx+FoihHgR1McgZ4CJwpPtgnrZFzwYsjNRpi37y+XqBOAhLsQ5rNZjT3v7BRjFEtVkBdmnGwTEAbt+hpT6Ub3Ne636yNniop6SbgLYYaibPjpZaN//NgeqHTMy+/lC1E9ofNIiE4wQjw6AUI7SveKOCsS7kI0pxPF8Mvrxs1+ArpdAj3HOkK8rzE0US5BJ5qAhLsQzcFeAb9/YMzXUnIUEq6Gy56ASLkAuXANCXchXElr2LUK1s6B3DToNBKmfwRxw8yuTHg4CXchXOXgBvj6MTi0wbhQ9PSPjROIpO9cNAMJdyGa2rFUY0991yoIjjauJzroRvCWf26i+cinTYimUnzU6FPfNB98A40LW5x/j1yCTphCwl2IxjpRDL++AT+/ZoyASbzNuLiFzKwoTCThLsS5sttg8wfGBaNlBIxwM17ONFJKjVNK7VZKpSmlZp+h3RSllFZKNXhlbiFarEo77FwF/28ErPqTEea3r4WpH0iwC7fR4J67UsobeBO4AsgANiqlVmitU2q1CwHuAza4olAhmpXWUJprDF88lmr8rLrl7QO71TiTVEbACDflTLfMeUCa1nofgFJqEXA1kFKr3dPAXOChJq1QCFeylkLe3pPBfaxGiJcfP9nOyxciukJkPMSPgZiBkHCNjIARbsuZT2ZH4FCNxxnA8JoNlFKDgTit9SqllIS7cE92G2z5CLK2nAzympNzgTGHS2R36DfZCPLIHhDVw5h9UYJctCDOfFrr+r6pqxcq5QW8AsxocEVK3QncCdCpUyfnKhSiKeTth2UzIWMj+IcZgd3lAiO8q2/dZdii8BjOhHsGEFfjcSxwuMbjEKAfsE4Z/Y7tgRVKqYla66SaK9JazwPmASQmJmqEaA5bFsMXfwblBde+C/2ulT5y4fGcCfeNQLxSqiuQCUwH/li1UGtdAERVPVZKrQMeqh3sQjS78gIj1Ld9Ap3Oh8nzIFy+MYrWocFw11rblFKzgDWAN/Ce1nqHUuopIElrvcLVRQpx1g6uN7phCjLhkkdh1INyhSLRqjh1hEhrvRpYXeu5x+tpO7rxZQlxjuw2+OEf8MNcCIuD29bIDIyiVZLD/8Jz5KfDsjuNWRgHTIcJ/5DL0IlWS8JdeIatS4z+dTAOmvafYm49QphMwl20bOUF8MVDsG0JxI0wDpq26Wx2VUKYTsJdtFwHN8CyO4yDpqP/DqP+LCcaCeEg/xJEy3PaQdP/Qtx5ZlclhFuRcBctS/4BY4jjoQ0wYBpMeFEOmgpRBwl30XJs/QS+eNC4P/kdGHCdufUI4cYk3IV7s5bC7tWQvBD2fgtxwx0HTbuYXZkQbk3CXbgfuw32f28Mb9y5EipKjNkaL3sCRt4nB02FcIL8KxHuQWs4vNmYB2bbUuOydQFhxnj1AVOh00jwcurCYUIIJNyF2fL2G4G+dQnkpoK3H/QcaxwsjR8DPv5mVyhEiyThLppfSS7sWGYEesZvxnNdRsHIeyFhIgS2Mbc+ITyAhLtoHlUHRrcugb3fQKUN2vWFy580ul7CYs2uUAiPIuEuXCtrK6z/F+xcAdZi48Do+fdA/6nQvp/Z1QnhsSTchWtkp8C654xQ9w81rknafyp0vkAOjArRDCTcRdPK2Q3rnocdn4FfMFz8MIy4GwLDza5MiFZFwl00jWOp8P0LxjBGvyBjEq/z7wFLhNmVCdEqSbiLxsnda0zitXUx+ATABfcbJxoFRZpdmRCtmoS7ODf56UaoJ38M3r5G18sFD0BwW7MrE0Ig4S7O1vFD8OOLsPlDUN5w3p1w4QMQ0t7syoQQNUi4C+cUZMJPL8Om90EpSLwNLvwThHYwuzIhRB0k3MWZFR2BH1+GTfNBV8KQm4yDpXLSkRBuTcJd1K3wMPzyOiS9B/YKGHwDjHpIrk8qRAsh4S5OlbsXfn4VtnwMlXZjAq+L/wIR3cyuTAhxFiTchSF7h9H9smMZePnCkJuNibzkohhCtEgS7q3doY3w40uw50vjjNKR98KIeyAk2uzKhBCNIOHeGmkN+9YZoZ7+ozHF7iWPwHkzZbpdITyEhHtrUllpTLv740tw+HcIiYGx/wdDbgH/YLOrE0I0IQn31sBug+2fGuPUc3YZ/ehXvQoDr5crHQnhoSTcPVlFOSQvNEa/HD8A7RLg2nch4Rq5yLQQHk7+hXsiawlsfBd+fQOKs6FjIox/AeLHylzqQrQSEu6eJv8AfDwdjqZAt9Fw7TvG9UmVMrsyIUQzknD3JAc3wKI/QmUF3Pgp9Ljc7IqEECaRcPcUW5fA5/cYc778cQlExZtdkRDCRBLuLV1lpXGt0h/mGt0vUz+Qqx8JIXDq6JpSapxSardSKk0pNbuO5XcppbYppZKVUj8ppRKavlRxGmspLL3VCPbBN8GNyyTYhRCAE+GulPIG3gTGAwnA9XWE90da6/5a60HAXODlJq9UnKroCMz/A6R8DmOegYmvg4+f2VUJIdyEM90y5wFpWut9AEqpRcDVQEpVA611YY32QYBuyiJFLVlb4OProew4TP8Iek8wuyIhhJtxJtw7AodqPM4AhtdupJS6B3gQ8AMubZLqxOl2roJlMyEwAm5fA+37m12REMINOdPnXtcA6dP2zLXWb2qtuwMPA4/WuSKl7lRKJSmlknJycs6u0tZOa/jpn7D4RmjXB2Z+K8EuhKiXM+GeAcTVeBwLHD5D+0XANXUt0FrP01onaq0T27Zt63yVrZ3NCp/PgrVPQN9JMOMLmZJXCHFGzoT7RiBeKdVVKeUHTAdW1GyglKo5qPoPQGrTldjKleTCgmsg+UO4eDZMeQ98A82uSgjh5hrsc9da25RSs4A1gDfwntZ6h1LqKSBJa70CmKWUuhyoAPKBW1xZdKuRsxs+mgqFWcaEX/2nmF2REKKFcOokJq31amB1recer3H//iauS+z9FpbMMIY3zvgC4oaZXZEQogWRKQLd0cZ34MMpxlQCM7+VYBdCnDWZfsCd2G3w1SOw4S1jet4p74J/iNlVCSFaIAl3d3GiCJbeBqlfGReoHvM0eHmbXZUQooWScHcHBZnw0TRjDvYrX4HE28yuSAjRwkm4m+1wsnFxjRPFcMMSmYNdCNEkJNzNtPtLoyvGEgm3fwXRMpmmEKJpyGgZM2gN6/9lTP7Vthfc8Y0EuxCiScmee3Oz22DN3+C3edD7Spj8b/CzmF2VEMLDSLg3p5ojYkbeC5c/BV7y5UkI0fQk3JuLjIgRQjQjCffmcDjZCHZriYyIEUI0Cwl3V5MRMUIIE0iHr6vIiBghhIlkz90V7Db472zY+G8ZESOEMIWEe1OTETFCCDcg4d6UThkR809IvNXsioQQrZSEe1M5ZY6YT6DHZWZXJIRoxaS/oLG0ht/+De+OAS8fY0SMBLsQwmSy594YpXmw4l7YtQp6XAHX/AuC25pdlRBCSLifswO/wKd3QPFRGPt/MPx/5cCpEMJtSLifrUo7/PAP+P4FaNMF7vgaOgw2uyohhDiFhPvZKMiEZTPhwM8wYDr84UW5xqkQwi1JuDtr1xfw+T1gr4BJb8PA6WZXJIQQ9ZJwb0hFOXz9mDH/esxAmPIfiOxudlVCCHFGEu5nkrPHONs0exucPwsuexx8/M2uSgghGiThXhetYfMC+PJh8A2EP34CPceYXZUQQjhNwr228gJY+QDsWAZdL4JJ8yA0xuyqhBDirEi415SRZHTDFGQYXTAXPABe3mZXJYQQZ03CHaCyEn55Fb59BkI6wG3/hbjzzK5KCCHOmYR7SS58ejvs+w4SroGrXoXAcLOrEkKIRmnd4V5RbszkeGSrEepDbgGlzK5KCCEarfWGu9bGpF8Zv8HUBZAw0eyKhBCiybTema5+fAm2LYFLH5NgF8JNaK0ptdooKq8wu5QWr3XuuaesgG+fhv5TYdSfza5GCI9kBLWdvBIrx0sryCu1kl9iJd/xM6/USn5phXG/RhurrRJvL8X9l8VzzyU98PaSrtJz0frC/XAyfPY/EDsMJr4ufexCNEJReQWpR4tJzS5iT3YxqUeLOVpYbgR4aQVWW2Wdr1MKwgN9aRPkR4TFj9g2FgbE+tLG4kebID+2ZRbw8td7+DntGK9MG0SH8MBm/s0MOw4X8Po3afj5eNEl0kKnyCDHTwttg/1RbpwfToW7Umoc8CrgDbyjtX6+1vIHgTsAG5AD3Ka1PtDEtTZeYRZ8fD0ERsD0j8A3wOyKhGgRSq020o4Wsye7mD3ZRezJLiI1u5jM42XVbQJ8vejRLpi4CAsDY8MJD/IlwuJXHdgRQb6EW4wwDw30PeMeudaaS3u14/HPtzP+1R954dr+jOvXfCcTllfYefWbVOb9sI/QAB+CA3xYtfUwlfpkG4ufN50iLHSOtNAlMohOVT8jLHQIDzT9G4fSWp+5gVLewB7gCiAD2Ahcr7VOqdHmEmCD1rpUKfW/wGit9bQzrTcxMVEnJSU1tn7nWUth/gRjvpjbv4L2/ZrvvYVoIcor7OzNqQpwY498d3YRGfllVEWFn48X3dsG0zM6mJ7RIY5bMLFtLE0eaOnHSrhv0Wa2ZhRw/XmdePzKBAL9XHti4YZ9ucxeto39x0qYmhjLIxMSCLP4YrVVknm8jPTcEg7mlp7y81BeGVb7yW8pvt6KuDaWUwK/a9sg+rQPJTq0cXv8SqlNWuvEhto5s+d+HpCmtd7nWPEi4GqgOty11t/VaL8euPHsynUxreHzu40umekfSbCLZqe15oStkpITNkqtdkqtdkqsNkIDfOkcacHXu3nHNmityTxexs6sInZmFbIzq5BdR4o4kFtSvXfq46Xo1jaIgbHhXDc0jp7RwcRHh9A5woJPM9XbJSqIpXeN5KWvd/P29/vYmJ7Ha9MHk9AhtMnfq6i8gue/3MXCDQfpFGFh4R3DuaBHVPVyPx8vukYF0TUq6LTXVlZqjhSW1wj8Ug7mlZB+rJSk9HyKT9iq27ax+JLQIZQ+7UPpE2PcerQLxs+nabepM+HeEThU43EGMPwM7W8HvmxMUU3u+xdgx2dwxVPQe4LZ1Qg3Z7VVUmqtCmEbJSeMIC6z2imx2ik9YaPEaqfMajvlcfVrHO1LrXZKTlS9znbKV/qavL0UnSMsdGsbRLe2wXR3/OwWFUREkF+j+3XLK+ykZhezM6uQFMdtV1YhheVG4CgFnSMs9G4fylUDO9AzOphe0SF0iQpq9v906uLn48XfxvdhVI+2PLgkmWve/Jm/TejNjJFdmqzPe21KNo8u387RonLuuLArD47picXP+UOSXl6KDuGBdAgPZGStGcG11uSVWEk7WsyuI0WkHC5k55FCFqw/wAnHMQlfb0X3tsEkxIQawe8I/Yggv3P+nZypvq6tV+fHVCl1I5AIXFzP8juBOwE6derkZImNtP1TWPccDLoBRt7XPO8p3NKhvFLe/yWdI4Xl1cFdFcBVe9OlVhsV9jN3Vdbk5+2Fxd+bID8fLH7ejpsPMWEBWKqf8yHI/+TPQF9vgvx9CPTzJr/Eyr6cEvYdK2bv0RJ+SD12ykHIcIsv3aKqQj+Ybm2D6N42iE4RQXXu6R0tKj9lbzzlcCH7jpVgd/zPYvHzpnf7EK4a2KE6QHq3DyHI3/3HVlwYH8WX94/ir0u38uTKFH5MPcY/pgwgMvjcp+E+VnyCOSt2sGprFr3bh/D2TUMZGNe0Z6grpYgM9icy2J/h3SKrn7fZK0nPLSElyxH4WYX8lHaMZZszq9tEh/pX/536xISSEOP8NxZn+tzPB+Zorcc6Hv8NQGv9XK12lwOvAxdrrY829MbN0ueescnoZ+8wGG7+XOZib6WOFpbz+rdpLNp4EKUUseGBWBxha/E7GcxB/qcG9KmBfOpji68PFn/vJt+ztVdqDh8vIy2n2Aj9nGL2Ou4fLTpR3c7bS9EpwkK3qCA6hAeSnlvCzqxCjhWygvG/AAASMUlEQVRbq9t0DA+kT0zIKeHQOcKCVwsfWqi15oNfD/Ds6p2EBfry8tSBjIpve9br+GxzJk+tSqH0hJ17L+3B/1zcvcm7Rs5FbvGJ6v+gUxz/SacdLcbm+A/6wAtXOtXn7ky4+2AcUL0MyMQ4oPpHrfWOGm0GA0uBcVrrVGd+AZeHe0Em/PsSI9BnfgdBUQ2/RniU/BIrb32/l/d/Tcdm10wbFse9l8bTPqxljpIqKq+o3svfl1NSHfqZx8voHGk5pQ+3T0wI4ZZz/0rfEuzMKuS+jzeTerSYOy/qxkNjejkVzhn5pfz9s+38sCeHoZ3b8MK1/enRzr2vhXzCZiftaDE7s4q4LjGuacIdQCk1AfgnxlDI97TWzyqlngKStNYrlFJrgf5AluMlB7XWZzzt06Xhbi2B98ZB3n6442to18c17yPcUvEJG+/+uJ93ftxHsdXGpEEdeeDynnSKtJhdmmhiZVY7z3yRwsINB+nfMYzXrh9c5wFPML4VffBrOv9YsxsF/HVcb24a0bnFfZNxdrSMU+HuCi4L98pK+ORm44LWf1wC8Vc0/XsIt1ReYWfBrwf41/d7ySuxMrZvNH8e04ue0e69VyYab82OIzz86VastkqenNiXKUNjTznYmppdxF8/3crmg8cZ3astz07qT0eTToxqrKYcCtmyfPcs7FwJY5+TYG8lKuyVLEk6xGvfpJJdeIJR8VE8NKZXkx8YE+5rbN/2DIgN40+Lk/nL0q38kHqMZyf1I8DHm3+t28ub36UR5O/NK9MGcs2gjm59ZmlT8axw37IYfnzRmLp3xP+aXU2rVFBWQWZ+GRn5pWQeLyMjv4zjpRV0axtEgqM/uLEncVSxV2pWbMnkla9TOZhXytDObfjntMGc3z2y4RcLjxMTFsjCO0bw1vd7efnrPWw+mE+Qnw+7s4uYOLADT1yV0KiRNS2N54T7od9gxSzoMgomvChzxriA1pr80tPDO6PG46Jy2ymvCfD1IizQl09/z6h+rrEncWit+Solm5e+2s2e7GISYkL5z4xhjO7VtlXskYn6eXsp7rmkByO7R3L/omQKyyt4b0Yil/aONru0ZucZfe7HD8K/LwX/ELjjG7BENM16WzF7pWbZ7xlsyTjuCPMyMo+XUWq1n9Iu2N+H2DaBdAwPNH62CSS2jaX6cdVJOIXlFeyqOf46q5DdR4pOOYmjR7sQ+sSEVO/h1z6JQ2vNT2nHeHHNbrZkFNAtKogHx/RkQr+YFndQTLhe1fkC7jC8sSm1ngOqJ4rg3bHGRa3vWAttezZ+na1c+rES/rJ0CxvT8wkL9CW2jSO4wy3VAd4xPJC4NhZCA33OeW+55kkcVSfc7MwqPGU8d/vQgOqx2r8fzGf9vjw6hgdy/+XxTB7csdlOgxfCXbSOA6qVdvh0JuTsghuXSrA3UmWlZsH6Azz/5S58vBUvTx3IpMGuO/jk4+1Fj3Yh9GgXwsSBHaqfr3kSR9Ve/o+pxwi3+DLnqgSuH94Jfx/XTh4lREvXssN97RzY86XRx979UrOradEy8kv569Kt/LI3l4t6tuWFa/sTE2bOULHIYH8ujPfnwviTJ55ZbZV4KWRPXQgntdxwP5YGv7wGQ2fAeTPNrqbF0lqzeOMhnvliJ1prnpvcn+nD4tzuwKSn9ZsK4WotN9yTPwTlDaP/bnYlLdaRgnJmL9vKut05nN8tkrlTBhAXIWdxCuEJWma4V9phyyLjJKWQ1jfEqbGqJk2as2IHFXbNkxP7tsjTsIUQ9WuZ4b73WyjKgvFzza6kxckpOsHfP9vG1ynZJHZuw4vXDaRLPXNxCCFarpYZ7ps/BEsk9BxndiVNxmavJC3HmNO7S5SFXtEhTX7wcNXWwzy2fDslVjuPTOjDbRd2Nf06j0II12h54V6aB7tXQ+Lt4NMypzS12StJPVrMtswCtmcWsC2zgJ1ZhZRXnLxIQ6CvN/07hjGoUziD4sIZ3Cn8nEev5JVYeezz7XyxNYuBsWG8NHWg209xKoRonJYX7tuWgt0Kg28wuxKn1AzybRkng7zqzMwgP2/6dgzjhuGd6d8xjO5tg9l3rJjkQ8dJPnSc+T+nV194NzrUn0Fx4QyKa8OguHAGxIY1eAWdr3Yc4e+fbaOgrIK/jO3F/1zUTYYTCtEKtLxwT/4Q2g+A9v3NruQ0FfZKUrOLq/fG6wvyG0cYQd4/NoyukUGnHcjsHxvG1YM6Asb47p1ZhdVhv/lgPmt2ZAPgpaBndIgj8MMZ1Cmc+HYheHspCkoreHLlDpZtzqRPTCgf3DbcJRcVFkK4p5YV7ke2Q9YWUw6kaq0pOmHjSEH5yVthOVkF5WQ7fu7LKT7rIG+In48XA+PCGRgXzi2O5/JLrCRnHCf5oBH4/91xhEUbD1W/b//YMNKPlZJTfIL7Lu3BrEvjZZy4EK1Mywr35IXg7Qf9r2vS1VZWao6VnCC74ARZBWXVYX2k8GSIHykoP23SLIDIID+iQwPoEBbAyO6RjQpyZ7UJ8uOSXu24pFc7wPiPJz23lORD+dWB3yE8gHk3D2VArMxpLkRr1HLC3WaFrYuh1/gmm/Uxt/gEL361m083ZVb3a1fx9lJEh/jTPiyA3u1DGN2zHe3D/GkfFkj70ABiwgJoF+rvFnOcKKXoGhVE16ggJg2ONbscIYQbaDnhnroGSnNh0I2NXlWFvZIFvx7glbV7KLPamTI0lj4xobQPC6gO7shgfxkmKIRosVpOuG9eCMHtGz1B2E+px3hy5Q5SjxYzKj6KJ65KkGGBQgiP0zLCvSgbUr+CkfeC97mVfCivlGe+SGHNjmw6RViYd9NQrkiIdrsJsoQQoim0jHDfuhi0HQad/dj2UquNf63by9s/7MNbKf4ythe3X9iVAF/z+8qFEMJV3D/ctTZGycQOO6uLcWitWbk1i+dW7ySroJxrBnVg9vg+tA8LcGGxQgjhHtw/3DN/N660dOU/nX7J9swCnlqZwm/pefTrGMrr1w8msYtcV1UI0Xq4f7gnfwg+gdBvcoNN80qsvPjVbj7+7SBtLH48N7k/UxPjZNSLEKLVce9wryiDbZ9Cn6sgIKzeZjZ7JR+uP8DLX++hxGrn1pFduf/yeMICfZuxWCGEcB/uHe67voATBWecJOznNGNo457sYi7sYQxtjI+WoY1CiNbNvcM9eSGEdYIuF53ytL1S81PaMT5cf4CvU7KJiwjk7ZuGMkaGNgohBODO4V6QAXu/g4v/Cl7GpFe7jxSx7PcMPtucydGiE4QF+vLQmJ7cMaqbDG0UQoga3Dfct3wMaPLip7D8p/18+nsGOw4X4uOluKR3O64d0pFLerdzi7ldhBDC3bhluJdbbdg3LOBQwED+8P/SsFdq+ncMY85VCVw1sAORwf5mlyiEEG7NbcJda83vB4+z7PcMMrd8w3wOsMh7IjNHdWPykI70lIOkQgjhNNPD/VBeKcs3Z7Jscyb7j5UQ4OvF+xHrsZUE8dhDs/EOCDa7RCGEaHFMC/f8UivT5/3K+n15AIzoFsHdo7szvlcIwa/dDv0ngwS7EEKcE9PCPSO/jE6FJ/jzFT2ZNKQjsW0sxoLNC6GiBAY3ft52IYRorZy6sKZSapxSardSKk0pNbuO5RcppX5XStmUUlOcWWf3tsF8++eLufey+JPBDsbY9sgeEDfc2d9BCCFELQ2Gu1LKG3gTGA8kANcrpRJqNTsIzAA+cvaNLX7ep59wlLcPDvwMg/4IcjKSEEKcM2e6Zc4D0rTW+wCUUouAq4GUqgZa63THssq6VuC05I9AecHA6xu1GiGEaO2c6ZbpCByq8TjD8VzTqrRD8sfGZfRCOzT56oUQojVxJtzr6h/R5/JmSqk7lVJJSqmknJycUxfu/x4KM87paktCCCFO5Uy4ZwBxNR7HAofP5c201vO01ola68S2bdueunDzQggIh14TzmXVQgghanAm3DcC8UqprkopP2A6sKJJqyg7DrtWQf/rwFcugyeEEI3VYLhrrW3ALGANsBNYorXeoZR6Sik1EUApNUwplQFcB7ytlNpxVlVs/xRs5cYoGSGEEI3m1ElMWuvVwOpazz1e4/5GjO6ac5O8ENolQIfB57wKIYQQJzl1EpNLHd0FmZuMA6kytl0IIZqE+eGe/CF4+cCAaWZXIoQQHsPccLdXwJbFED8Wgts23F4IIYRTzA33tLVQcvSMF8AWQghx9swN980fQlBbiB9jahlCCOFpzAv3Shvs+a/R1+7ta1oZQgjhicwL97J8I+BlugEhhGhy5oV7aa4xrj269uzBQgghGsu8cK8ok712IYRwEfPCXXlBf6cu2iSEEOIsmRfuQVEQ2Ma0txdCCE9mXriHNv31PoQQQhjMn35ACCFEk5NwF0IIDyThLoQQHkjCXQghPJCEuxBCeCAJdyGE8EAS7kII4YEk3IUQwgMprbU5b6xUEbDblDc/N1HAMbOLOAtSr2tJva4l9davs9a6wUvX+TRHJfXYrbVONPH9z4pSKknqdR2p17WkXtdyx3qlW0YIITyQhLsQQnggM8N9nonvfS6kXteSel1L6nUtt6vXtAOqQgghXEe6ZYQQwgO5PNyVUuOUUruVUmlKqdl1LPdXSi12LN+glOri6prqo5SKU0p9p5TaqZTaoZS6v442o5VSBUqpZMftcTNqrVFPulJqm6OWpDqWK6XUa47tu1UpNcSMOh219Kqx3ZKVUoVKqQdqtTF1+yql3lNKHVVKba/xXIRS6mulVKrjZ51XmVFK3eJok6qUusXEev+hlNrl+Ht/ppQKr+e1Z/zsNGO9c5RSmTX+5hPqee0Zs6QZ611co9Z0pVRyPa9t9u17Cq21y26AN7AX6Ab4AVuAhFpt7gbectyfDix2ZU0N1BsDDHHcDwH21FHvaGCVWTXWUXM6EHWG5ROALwEFjAA2mF1zjc/GEYwxu26zfYGLgCHA9hrPzQVmO+7PBl6o43URwD7HzzaO+21MqncM4OO4/0Jd9Trz2WnGeucADznxeTljljRXvbWWvwQ87i7bt+bN1Xvu5wFpWut9WmsrsAi4ulabq4H3HfeXApcppZSL66qT1jpLa/27434RsBNo6ZeMuhr4QBvWA+FKqRiziwIuA/ZqrQ+YXUhNWusfgLxaT9f8jL4PXFPHS8cCX2ut87TW+cDXwDiXFepQV71a66+01jbHw/VArKvrcFY929cZzmRJkztTvY6cmgp87Oo6zoWrw70jcKjG4wxOD8vqNo4PZAEQ6eK6GuToHhoMbKhj8flKqS1KqS+VUn2btbDTaeArpdQmpdSddSx35m9ghunU/4/CnbYvQLTWOguMHQCgXR1t3HU734bxza0uDX12mtMsRzfSe/V0e7nj9h0FZGutU+tZbur2dXW417UHXnt4jjNtmpVSKhj4FHhAa11Ya/HvGF0JA4HXgeXNXV8tF2ithwDjgXuUUhfVWu6O29cPmAh8Usdid9u+znLH7fwIYAMW1tOkoc9Oc/kX0B0YBGRhdHXU5nbbF7ieM++1m7p9XR3uGUBcjcexwOH62iilfIAwzu1rW5NQSvliBPtCrfWy2su11oVa62LH/dWAr1IqqpnLrFnPYcfPo8BnGF9fa3Lmb9DcxgO/a62zay9wt+3rkF3VleX4ebSONm61nR0HdK8EbtCODuDanPjsNAutdbbW2q61rgT+XU8d7rZ9fYDJwOL62pi9fV0d7huBeKVUV8fe2nRgRa02K4CqkQVTgG/r+zC6mqMP7V1gp9b65XratK86JqCUOg9jG+Y2X5Wn1BKklAqpuo9xIG17rWYrgJsdo2ZGAAVVXQwmqnePx522bw01P6O3AJ/X0WYNMEYp1cbRrTDG8VyzU0qNAx4GJmqtS+tp48xnp1nUOgY0qZ46nMmS5nQ5sEtrnVHXQrfYvs1wtHkCxqiTvcAjjueewvjgAQRgfD1PA34Dupl1dBm4EOOr3lYg2XGbANwF3OVoMwvYgXG0fj0w0sR6uznq2OKoqWr71qxXAW86tv82INGseh31WDDCOqzGc26zfTH+08kCKjD2Fm/HOAb0DZDq+BnhaJsIvFPjtbc5PsdpwK0m1puG0T9d9RmuGo3WAVh9ps+OSfUucHw2t2IEdkzteh2PT8sSM+p1PD+/6jNbo63p27fmTc5QFUIIDyRnqAohhAeScBdCCA8k4S6EEB5Iwl0IITyQhLsQQnggCXchhPBAEu5CCOGBJNyFEMID/X9OrDKkB/EsNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history, columns=[\"avg_train_loss\", \"avg_train_acc\", \"avg_valid_loss\", \"avg_valid_acc\"])[[\"avg_valid_acc\", \"avg_train_acc\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_image_name):\n",
    "     \n",
    "    transform = image_transforms['test']\n",
    " \n",
    "    test_image = Image.open(test_image_name)\n",
    "    plt.imshow(test_image)\n",
    "     \n",
    "    test_image_tensor = transform(test_image)\n",
    " \n",
    "    if torch.cuda.is_available():\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
    "    else:\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_image_tensor)\n",
    "        ps = torch.exp(out)\n",
    "        topk, topclass = ps.topk(1, dim=1)\n",
    "        print(\"Output class :  \", idx_to_class[topclass.cpu().numpy()[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class CustomDatasetFromCSV(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.labels = pd.get_dummies(self.data['emotion']).as_matrix()\n",
    "        self.height = 48\n",
    "        self.width = 48\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # This method should return only 1 sample and label \n",
    "        # (according to \"index\"), not the whole dataset\n",
    "        # So probably something like this for you:\n",
    "        pixel_sequence = self.data['pixels'][index]\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        face = np.asarray(face).reshape(self.width, self.height)\n",
    "        face = cv2.resize(face.astype('uint8'), (self.width, self.height))\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return face, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "dataset = CustomDatasetFromCSV(my_path)\n",
    "batch_size = 16\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "# Usage Example:\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train:   \n",
    "    for batch_index, (faces, labels) in enumerate(train_loader):\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "            \n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        dataset_type = type(dataset)\n",
    "        if dataset_type is torchvision.datasets.MNIST:\n",
    "            return dataset.train_labels[idx].item()\n",
    "        elif dataset_type is torchvision.datasets.ImageFolder:\n",
    "            return dataset.imgs[idx][1]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_dir = \"alien_pred\"\n",
    "input_shape = 224\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "#data transformation\n",
    "data_transforms = {\n",
    "   'train': transforms.Compose([\n",
    "       transforms.CenterCrop(input_shape),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean, std)\n",
    "   ]),\n",
    "   'validation': transforms.Compose([\n",
    "       transforms.CenterCrop(input_shape),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean, std)\n",
    "   ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "   x: datasets.ImageFolder(\n",
    "       os.path.join(data_dir, x),\n",
    "       transform=data_transforms[x]\n",
    "   )\n",
    "   for x in ['train', 'validation']\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "   x: torch.utils.data.DataLoader(\n",
    "       image_datasets[x], batch_size=32,\n",
    "       shuffle=True, num_workers=4\n",
    "   )\n",
    "   for x in ['train', 'validation']\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation']}\n",
    "\n",
    "print(dataset_sizes)\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(vgg_based.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "   since = time.time()\n",
    "\n",
    "   for epoch in range(num_epochs):\n",
    "       print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "       print('-' * 10)\n",
    "\n",
    "       #set model to trainable\n",
    "       # model.train()\n",
    "\n",
    "       train_loss = 0\n",
    "\n",
    "       # Iterate over data.\n",
    "       for i, data in enumerate(dataloaders['train']):\n",
    "           inputs , labels = data\n",
    "           inputs = inputs.to(device)\n",
    "           labels = labels.to(device)\n",
    "\n",
    "           optimizer.zero_grad()\n",
    "          \n",
    "           with torch.set_grad_enabled(True):\n",
    "               outputs  = model(inputs)\n",
    "               loss = criterion(outputs, labels)\n",
    "\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "\n",
    "           train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "           print('{} Loss: {:.4f}'.format(\n",
    "               'train', train_loss / dataset_sizes['train']))\n",
    "          \n",
    "   time_elapsed = time.time() - since\n",
    "   print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "       time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "   return model\n",
    "\n",
    "def visualize_model(model, num_images=6):\n",
    "   was_training = model.training\n",
    "   model.eval()\n",
    "   images_so_far = 0\n",
    "   fig = plt.figure()\n",
    "\n",
    "   with torch.no_grad():\n",
    "       for i, (inputs, labels) in enumerate(dataloaders['validation']):\n",
    "           inputs = inputs.to(device)\n",
    "           labels = labels.to(device)\n",
    "\n",
    "           outputs = model(inputs)\n",
    "           _, preds = torch.max(outputs, 1)\n",
    "\n",
    "           for j in range(inputs.size()[0]):\n",
    "               images_so_far += 1\n",
    "               ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "               ax.axis('off')\n",
    "               ax.set_title('predicted: {} truth: {}'.format(class_names[preds[j]], class_names[labels[j]]))\n",
    "               img = inputs.cpu().data[j].numpy().transpose((1, 2, 0))\n",
    "               img = std * img + mean\n",
    "               ax.imshow(img)\n",
    "\n",
    "               if images_so_far == num_images:\n",
    "                   model.train(mode=was_training)\n",
    "                   return\n",
    "       model.train(mode=was_training)\n",
    "    \n",
    "vgg_based = train_model(vgg_based, criterion, optimizer_ft, num_epochs=25)\n",
    "\n",
    "visualize_model(vgg_based)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
