{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Load_Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = \"data/cars_train/\"\n",
    "car_annotations_path = \"data/devkit/cars_train_annos.mat\"\n",
    "car_metadata_path = \"data/devkit/cars_meta.mat\"\n",
    "\n",
    "#Load Meta Data\n",
    "meta_data = loadmat(car_metadata_path)\n",
    "meta_data = np.concatenate(meta_data[\"class_names\"][0])\n",
    "\n",
    "nb_classes = len(meta_data)\n",
    "\n",
    "dataset = Load_Images(root_dir = root_dir, annotations_path=car_annotations_path, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, class_id = sample['Image'], sample['class_ID']\n",
    "        image = np.asarray(image)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        \n",
    "        tensor = torch.from_numpy(np.array(image/255).astype(np.float32))\n",
    "        \n",
    "        \n",
    "        return {'Image': tensor,\n",
    "                'class_ID': target,\n",
    "                'class_name': sample[\"class_name\"]}\n",
    "\n",
    "class car_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, files, root_dir, meta_data, image_transform=None):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.image_transform = image_transform\n",
    "        \n",
    "        #image file names\n",
    "        self.image_files = [file[-1][0] for file in files]\n",
    "        \n",
    "        #Class ID\n",
    "        #id needs to be adjusted by 1, for pytorch NLLosss \n",
    "        self.id = [file[-2][0] - 1 for file in files]\n",
    "        \n",
    "        #Class Name\n",
    "        self.class_name = [meta_data[file[-2][0] - 1][0] for file in files]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        if self.image_transform:\n",
    "            img = self.image_transform(img)\n",
    "        \n",
    "        target = torch.from_numpy(np.array(self.id[idx]))[0]\n",
    "\n",
    "        sample = {'Image':img, 'class_ID':target, \"class_name\":self.class_name[idx]}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Training Weighted Random Sampler\n",
    "targets = [i[-2][0][0] for i in dataset[\"training\"]]\n",
    "class_sample_counts=[len(np.where(targets == t)[0]) for t in np.unique(targets)]\n",
    "weight = 1. / np.array(class_sample_counts)\n",
    "samples_weight = np.array([weight[t-1] for t in targets])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weight = samples_weight.double()\n",
    "training_sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "\n",
    "bs = 15\n",
    "\n",
    "image_transformers = {'train': transforms.Compose([transforms.Resize((244,244)),\n",
    "                                                   transforms.RandomRotation(degrees=15),\n",
    "                                                   transforms.RandomHorizontalFlip(0.5),\n",
    "                                                   transforms.ColorJitter(brightness=0.5, contrast=0.3),\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                        [0.229, 0.224, 0.225])\n",
    "                                                  ]),\n",
    "                      'validation': transforms.Compose([transforms.Resize((244,244)),\n",
    "                                                       transforms.ToTensor(),\n",
    "                                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                            [0.229, 0.224, 0.225])\n",
    "                                                       ]),\n",
    "                      'test':transforms.Compose([transforms.Resize((244,244)),\n",
    "                                                       transforms.ToTensor(),\n",
    "                                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                            [0.229, 0.224, 0.225])\n",
    "                                                ])\n",
    "                     }\n",
    "\n",
    "#\n",
    "training_data = car_dataset(dataset[\"training\"],\n",
    "                            root_dir = root_dir,\n",
    "                            meta_data = meta_data,\n",
    "                            image_transform = image_transformers[\"train\"]\n",
    "                           )\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=bs, \n",
    "                                           sampler=training_sampler)\n",
    "\n",
    "validation_data = car_dataset(dataset[\"validation\"], \n",
    "                             root_dir = root_dir,\n",
    "                             meta_data = meta_data,\n",
    "                             image_transform  = image_transformers[\"validation\"])\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=bs,\n",
    "                                                shuffle=False)\n",
    "\n",
    "test_data = car_dataset(dataset[\"test\"], \n",
    "                             root_dir = root_dir,\n",
    "                             meta_data = meta_data,\n",
    "                             image_transform  = image_transformers[\"test\"])\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=bs,\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6500"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 960'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=196, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Load the model based on VGG19\n",
    "vgg_based = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "'''\n",
    "for param in vgg_based.parameters():\n",
    "    param.requires_grad = False\n",
    "'''\n",
    "\n",
    "\n",
    "for idx,param in enumerate(vgg_based.parameters()):\n",
    "    if idx <= 33:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "# Modify the last layer\n",
    "number_features = vgg_based.classifier[6].in_features\n",
    "features = list(vgg_based.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([torch.nn.Linear(number_features, nb_classes)])\n",
    "vgg_based.classifier = torch.nn.Sequential(*features)\n",
    "\n",
    "vgg_based = vgg_based.to(device)\n",
    "\n",
    "print(vgg_based)\n",
    "#torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0)\n",
    "#\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = torch.optim.Adam(vgg_based.parameters(), lr= 0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_ft = torch.optim.Adam(vgg_based.parameters(), lr= 0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=1):\n",
    "    since = time.time()\n",
    "    history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 30)\n",
    "\n",
    "        training_loss = 0\n",
    "        validation_loss = 0\n",
    "        \n",
    "        training_accuracy = 0\n",
    "        validation_accuracy = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            inputs = data[\"Image\"]\n",
    "            labels = data[\"class_ID\"]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs  = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            training_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            #train accuracy\n",
    "            (max_vals, arg_maxs) = torch.max(outputs, dim=1) \n",
    "            correct_counts = arg_maxs.eq(labels.data.view_as(arg_maxs))\n",
    "\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            training_accuracy += acc.item() * inputs.size(0)\n",
    "\n",
    "            \n",
    "        #get accuracy\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for batch_idx, data in enumerate(validation_loader):\n",
    "                inputs = data[\"Image\"]\n",
    "                labels = data[\"class_ID\"]\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.long()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                validation_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                #train accuracy\n",
    "                (max_vals, arg_maxs) = torch.max(outputs, dim=1) \n",
    "                correct_counts = arg_maxs.eq(labels.data.view_as(arg_maxs))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                validation_accuracy += acc.item() * inputs.size(0)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = training_loss/len(training_data)\n",
    "        avg_train_acc = training_accuracy/float(len(training_data))\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = validation_loss/len(validation_data)\n",
    "        avg_valid_acc = validation_accuracy/float(len(validation_data))\n",
    "        history.append([avg_train_loss, avg_train_acc, avg_valid_loss, avg_valid_acc])\n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%\".format(epoch + 1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "------------------------------\n",
      "Epoch : 001, Training: Loss: 3.5293, Accuracy: 19.5077%, \n",
      "\t\tValidation : Loss : 3.7453, Accuracy: 15.9314%\n",
      "Epoch 2/30\n",
      "------------------------------\n",
      "Epoch : 002, Training: Loss: 2.9308, Accuracy: 30.5385%, \n",
      "\t\tValidation : Loss : 3.6010, Accuracy: 21.0784%\n",
      "Epoch 3/30\n",
      "------------------------------\n",
      "Epoch : 003, Training: Loss: 2.4882, Accuracy: 38.2000%, \n",
      "\t\tValidation : Loss : 3.6593, Accuracy: 23.7745%\n",
      "Epoch 4/30\n",
      "------------------------------\n",
      "Epoch : 004, Training: Loss: 2.2515, Accuracy: 43.8000%, \n",
      "\t\tValidation : Loss : 3.6084, Accuracy: 20.8333%\n",
      "Epoch 5/30\n",
      "------------------------------\n",
      "Epoch : 005, Training: Loss: 1.9658, Accuracy: 49.2923%, \n",
      "\t\tValidation : Loss : 3.6744, Accuracy: 24.0196%\n",
      "Epoch 6/30\n",
      "------------------------------\n",
      "Epoch : 006, Training: Loss: 1.7497, Accuracy: 54.0000%, \n",
      "\t\tValidation : Loss : 3.9025, Accuracy: 24.5098%\n",
      "Epoch 7/30\n",
      "------------------------------\n",
      "Epoch : 007, Training: Loss: 1.6045, Accuracy: 57.2923%, \n",
      "\t\tValidation : Loss : 3.9419, Accuracy: 25.0000%\n",
      "Epoch 8/30\n",
      "------------------------------\n",
      "Epoch : 008, Training: Loss: 1.5081, Accuracy: 59.9692%, \n",
      "\t\tValidation : Loss : 3.9952, Accuracy: 25.2451%\n",
      "Epoch 9/30\n",
      "------------------------------\n",
      "Epoch : 009, Training: Loss: 1.3689, Accuracy: 63.0923%, \n",
      "\t\tValidation : Loss : 4.2436, Accuracy: 24.5098%\n",
      "Epoch 10/30\n",
      "------------------------------\n",
      "Epoch : 010, Training: Loss: 1.2679, Accuracy: 64.9846%, \n",
      "\t\tValidation : Loss : 4.3327, Accuracy: 25.9804%\n",
      "Epoch 11/30\n",
      "------------------------------\n",
      "Epoch : 011, Training: Loss: 1.1901, Accuracy: 67.6923%, \n",
      "\t\tValidation : Loss : 4.2262, Accuracy: 23.2843%\n",
      "Epoch 12/30\n",
      "------------------------------\n",
      "Epoch : 012, Training: Loss: 1.1306, Accuracy: 69.7846%, \n",
      "\t\tValidation : Loss : 4.2415, Accuracy: 27.6961%\n",
      "Epoch 13/30\n",
      "------------------------------\n",
      "Epoch : 013, Training: Loss: 1.1031, Accuracy: 69.4154%, \n",
      "\t\tValidation : Loss : 4.2816, Accuracy: 24.7549%\n",
      "Epoch 14/30\n",
      "------------------------------\n",
      "Epoch : 014, Training: Loss: 1.0315, Accuracy: 71.6923%, \n",
      "\t\tValidation : Loss : 4.5927, Accuracy: 26.2255%\n",
      "Epoch 15/30\n",
      "------------------------------\n",
      "Epoch : 015, Training: Loss: 1.0012, Accuracy: 72.0769%, \n",
      "\t\tValidation : Loss : 4.7630, Accuracy: 28.4314%\n",
      "Epoch 16/30\n",
      "------------------------------\n",
      "Epoch : 016, Training: Loss: 0.9168, Accuracy: 74.6154%, \n",
      "\t\tValidation : Loss : 4.7119, Accuracy: 28.6765%\n",
      "Epoch 17/30\n",
      "------------------------------\n",
      "Epoch : 017, Training: Loss: 0.9114, Accuracy: 74.9385%, \n",
      "\t\tValidation : Loss : 4.9168, Accuracy: 27.9412%\n",
      "Epoch 18/30\n",
      "------------------------------\n",
      "Epoch : 018, Training: Loss: 0.8383, Accuracy: 76.5385%, \n",
      "\t\tValidation : Loss : 5.2150, Accuracy: 25.9804%\n",
      "Epoch 19/30\n",
      "------------------------------\n",
      "Epoch : 019, Training: Loss: 0.7978, Accuracy: 77.8615%, \n",
      "\t\tValidation : Loss : 5.4163, Accuracy: 28.1863%\n",
      "Epoch 20/30\n",
      "------------------------------\n",
      "Epoch : 020, Training: Loss: 0.8085, Accuracy: 77.5077%, \n",
      "\t\tValidation : Loss : 5.5529, Accuracy: 26.9608%\n",
      "Epoch 21/30\n",
      "------------------------------\n",
      "Epoch : 021, Training: Loss: 0.7533, Accuracy: 79.1231%, \n",
      "\t\tValidation : Loss : 5.8033, Accuracy: 25.0000%\n",
      "Epoch 22/30\n",
      "------------------------------\n",
      "Epoch : 022, Training: Loss: 0.7520, Accuracy: 79.0308%, \n",
      "\t\tValidation : Loss : 5.6110, Accuracy: 25.0000%\n",
      "Epoch 23/30\n",
      "------------------------------\n",
      "Epoch : 023, Training: Loss: 0.7511, Accuracy: 79.1231%, \n",
      "\t\tValidation : Loss : 5.8127, Accuracy: 27.2059%\n",
      "Epoch 24/30\n",
      "------------------------------\n",
      "Epoch : 024, Training: Loss: 0.7240, Accuracy: 80.0308%, \n",
      "\t\tValidation : Loss : 5.8491, Accuracy: 26.4706%\n",
      "Epoch 25/30\n",
      "------------------------------\n",
      "Epoch : 025, Training: Loss: 0.7129, Accuracy: 81.1538%, \n",
      "\t\tValidation : Loss : 6.2676, Accuracy: 25.4902%\n",
      "Epoch 26/30\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-05b90abcb552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvgg_based\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-f0b395b7faeb>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m# Compute the total loss for the batch and add it to valid_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mtraining_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;31m#train accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mods, history = train_model(vgg_based, criterion, optimizer_ft, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history).to_csv(\"jol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "# Usage Example:\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train:   \n",
    "    for batch_index, (faces, labels) in enumerate(train_loader):\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_image_name):\n",
    "     \n",
    "    transform = image_transforms['test']\n",
    " \n",
    "    test_image = Image.open(test_image_name)\n",
    "    plt.imshow(test_image)\n",
    "     \n",
    "    test_image_tensor = transform(test_image)\n",
    " \n",
    "    if torch.cuda.is_available():\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
    "    else:\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_image_tensor)\n",
    "        ps = torch.exp(out)\n",
    "        topk, topclass = ps.topk(1, dim=1)\n",
    "        print(\"Output class :  \", idx_to_class[topclass.cpu().numpy()[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class CustomDatasetFromCSV(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.labels = pd.get_dummies(self.data['emotion']).as_matrix()\n",
    "        self.height = 48\n",
    "        self.width = 48\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # This method should return only 1 sample and label \n",
    "        # (according to \"index\"), not the whole dataset\n",
    "        # So probably something like this for you:\n",
    "        pixel_sequence = self.data['pixels'][index]\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        face = np.asarray(face).reshape(self.width, self.height)\n",
    "        face = cv2.resize(face.astype('uint8'), (self.width, self.height))\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return face, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "dataset = CustomDatasetFromCSV(my_path)\n",
    "batch_size = 16\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "# Usage Example:\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train:   \n",
    "    for batch_index, (faces, labels) in enumerate(train_loader):\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "            \n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        dataset_type = type(dataset)\n",
    "        if dataset_type is torchvision.datasets.MNIST:\n",
    "            return dataset.train_labels[idx].item()\n",
    "        elif dataset_type is torchvision.datasets.ImageFolder:\n",
    "            return dataset.imgs[idx][1]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_dir = \"alien_pred\"\n",
    "input_shape = 224\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "#data transformation\n",
    "data_transforms = {\n",
    "   'train': transforms.Compose([\n",
    "       transforms.CenterCrop(input_shape),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean, std)\n",
    "   ]),\n",
    "   'validation': transforms.Compose([\n",
    "       transforms.CenterCrop(input_shape),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean, std)\n",
    "   ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "   x: datasets.ImageFolder(\n",
    "       os.path.join(data_dir, x),\n",
    "       transform=data_transforms[x]\n",
    "   )\n",
    "   for x in ['train', 'validation']\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "   x: torch.utils.data.DataLoader(\n",
    "       image_datasets[x], batch_size=32,\n",
    "       shuffle=True, num_workers=4\n",
    "   )\n",
    "   for x in ['train', 'validation']\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation']}\n",
    "\n",
    "print(dataset_sizes)\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(vgg_based.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "   since = time.time()\n",
    "\n",
    "   for epoch in range(num_epochs):\n",
    "       print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "       print('-' * 10)\n",
    "\n",
    "       #set model to trainable\n",
    "       # model.train()\n",
    "\n",
    "       train_loss = 0\n",
    "\n",
    "       # Iterate over data.\n",
    "       for i, data in enumerate(dataloaders['train']):\n",
    "           inputs , labels = data\n",
    "           inputs = inputs.to(device)\n",
    "           labels = labels.to(device)\n",
    "\n",
    "           optimizer.zero_grad()\n",
    "          \n",
    "           with torch.set_grad_enabled(True):\n",
    "               outputs  = model(inputs)\n",
    "               loss = criterion(outputs, labels)\n",
    "\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "\n",
    "           train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "           print('{} Loss: {:.4f}'.format(\n",
    "               'train', train_loss / dataset_sizes['train']))\n",
    "          \n",
    "   time_elapsed = time.time() - since\n",
    "   print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "       time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "   return model\n",
    "\n",
    "def visualize_model(model, num_images=6):\n",
    "   was_training = model.training\n",
    "   model.eval()\n",
    "   images_so_far = 0\n",
    "   fig = plt.figure()\n",
    "\n",
    "   with torch.no_grad():\n",
    "       for i, (inputs, labels) in enumerate(dataloaders['validation']):\n",
    "           inputs = inputs.to(device)\n",
    "           labels = labels.to(device)\n",
    "\n",
    "           outputs = model(inputs)\n",
    "           _, preds = torch.max(outputs, 1)\n",
    "\n",
    "           for j in range(inputs.size()[0]):\n",
    "               images_so_far += 1\n",
    "               ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "               ax.axis('off')\n",
    "               ax.set_title('predicted: {} truth: {}'.format(class_names[preds[j]], class_names[labels[j]]))\n",
    "               img = inputs.cpu().data[j].numpy().transpose((1, 2, 0))\n",
    "               img = std * img + mean\n",
    "               ax.imshow(img)\n",
    "\n",
    "               if images_so_far == num_images:\n",
    "                   model.train(mode=was_training)\n",
    "                   return\n",
    "       model.train(mode=was_training)\n",
    "    \n",
    "vgg_based = train_model(vgg_based, criterion, optimizer_ft, num_epochs=25)\n",
    "\n",
    "visualize_model(vgg_based)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
